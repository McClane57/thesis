\section*{Introduction}\label{sec:reco-intro}
\addcontentsline{toc}{section}{Introduction}
In Chapter \ref{ch:soda}, we present\dg{ed the} general framework \spy~that is a version of \dave~\cite{ICML18} with sparsified communication. However, this random sparsification technique provably works only for i.i.d. sparsifications with either almost-uniform distributions, or well-conditioned problems. This makes aggressive sparsification or adaptation to the sparsity structure of the model impossible with such an algorithm.

The proximal point algorithm is a standard regularization approach in optimization. It was presented in \cite[Chap.\,5]{bkl} to recondition a convex quadratic objective, for which computing the proximal operator~\eqref{eq:prox} is easy (it is the unique solution of a linear system, well-conditioned by construction). The general proximal algorithm was then popularized by the seminal works\;\cite{martinet-1970,rockafellar1976monotone}. The study of these algorithms, and especially their inexact variants, has attracted a lot of attention; see e.g.\;\cite{guler1992new,solodov2000error,fuentes,lin2017catalyst,lin2019inexact}. 

Practical implementations of such methods require an inner algorithm to compute the proximal point and a rule to stop this algorithm. Several papers consider the key question of inner stopping criteria for inexact proximal methods in various contexts; see e.g.\;\cite{fuentes} in smooth optimization, \cite{lemarechal-sagastizabal-1997} in nonsmooth optimization,
and \cite{solodov-svaiter-2001} in operator theory.% In this paper, we use the standard criteria, following \cite{rockafellar1976monotone}, which gave the first inexact rules for the proximal point algorithm in the context of monotone operators.

Proximal reconditioning scheme wrapping up the previously mentioned algorithm as an inner minimization method allows us to perform much more aggressive sparsifications. Furthermore, we show that when using a sparsity-inducing regularizer, our reconditioned algorithm generates iterates that identify the optimal sparsity pattern of the model in finite time. This progressively uncovered pattern can be used to adaptively update the sparsification distribution of the inner method. All in all, this method only features sparse communications: the downward communications (master-to-worker) consists in sending the (eventually) sparse model, and the the upwards communications (worker-to-master) are adaptively and aggressively sparsified.

Finally, we show theoretically and numerically that our method has better performance than its non-sparsified version, in terms of suboptimality with respect to the quantity of information exchanged.

\paragraph{Outline}
This chapter is organized as follows. First, in Section \ref{sec:spy_recon}, we present a proximal reconditioning framework that allows wrapping up any optimization algorithm. We precise it to the case of \spyI~(see Algorithm \ref{alg:soda})  and investigate the convergence of such algorithm with three different stopping criterion. In Section \ref{sec:spy-identification}, we present an identification result for this method under the standard non-degeneracy assumption. It keeps the automatic-dimension reduction property of the inner algorithm while it converges; furthermore, we present an improved convergence rate based on this property. Finally, in Section \ref{sec:spy_exps}, we present the numerical experiments to prove that in practice algorithm performs better than \dave.

\dg{This chapter corresponds to \cite{grishchenko2018asynchronous}.}