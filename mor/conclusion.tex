\section{Conclusion}\label{sec:mor-conclusion}
{In this work, we propose a randomized proximal gradient method harnessing this underlying structure. We introduce two key components: i) a random subspace proximal gradient algorithm; ii) an identification-based sampling of the subspaces. Their interplay brings a significant performance improvement on typical learning problems in terms of dimensions explored.

One of the possible we study the convergence of the subspace descent algorithms, when the smooth function $f$ is convex but not strongly convex. Removing the strong convexity from Assumption~\ref{hyp:f}, we need the existence of the optimal solutions of \eqref{eq:main_problem} and thus we make the following assumption.

}