\section{Introduction}\label{sec:distributed-intro}

Given the tremendous production of data and the ever-growing size of collections, there is a surge of interest in both Machine Learning and Optimization communities for the development of efficient and scalable distributed learning strategies. In such context, training observations are generally split over different computing nodes, and learning is performed simultaneously where each node, also referred to as a \textit{worker}, has its memory and processing unit.  Note that this is different from shared-memory parallel computing, where each worker machine can potentially have access to all available memory \cite{Leslie91, Kumar02}. 

All distributed algorithms could be segregated according to the network structure. Some of them use networks with $1$ central node called master that has a LAN connection with all the other nodes called workers \cite{konevcny2016federated,konevcny2016federated2,mishchenko2018}. In such setups usually, all the data is split between worker machines, and worker machines update their parameters simultaneously on a local sub-part of data and send their updated parameters to a master machine. The master integrates all received parameters and broadcasts them back to each computing node for a new local update of their parameters. This setup is called a centralized or master-worker setup. It could be a different amount of layers: master communicates only with some of the other nodes that are the local masters and so on. Graphs of such networks are trees with the root being the master node, leaves being the workers, and all the others being the local masters. It is easy to see that from the theoretical point of view, such setup is quite the same as the first one.

There is another setup called decentralized, when all the nodes are the same and could communicate with neighbor nodes with respect to the network graph \cite{nedic2009distributed, boyd2011distributed, duchi2011dual, shi2015extra}. In this setup, the approach usually builds on the seminal work of Tsitsikas \cite{tsitsiklis1984problems} (see also \cite{bertsekas1997parallel} and \cite{tsitsiklis1986distributed}) who proposed a framework to analyze the distributed computation models. Another important part is the exact computing averages of values that are initially stored on the nodes \cite{boyd2011distributed, olshevsky2006convergence,olshevsky2009convergence}. 

In our work, we focus on the centralized distributed setup without shared memory \cite{mishchenko2018} where all the data $\data$ is separated between $M$ workers and all local subsets $\data_i$ are stored on each machine. {\color{blue}We also consider the case when there is only one layer, and all worker machines are connected with the central node of the system - master node. }This setup could be further categorized into two approaches with different assumptions on the communication rounds - synchronous and asynchronous. In the synchronous approach, every communication round mobilizes all worker machines \cite{BoydPCPE11,Chen2016,Tsianos12}. The study of distributed SGD in a star-shaped network \cite{yang2013trading} raises the question of the communication-computation tradeoff. {\color{blue} Every communication round in a synchronous setting consists of two phases: All-Reduce synchronization step when all the workers send their updates to the master and Broadcasting step that shares the updated parameter vector with all workers. This could significantly increase the working time of optimization algorithms when the amount of communication rounds, workers, and bits in the update are big due to the communication overhead.} It promotes the paradigm of mini-batch methods \cite{dekel2012optimal, shalev2013accelerated, shamir2014distributed, qu2016coordinate, takavc2015distributed}. The main idea of these methods in generalizing stochastic methods to process multiple data points on every iteration to make communication rounds less often. However, when the significant reduction of communication required, the size of mini-batch becomes big, which revert stochastic methods to the full-batch methods. Another way to look at this idea is local SGD methods that have been investigated in \cite{stich2018local,khaled2019first,khaled2020tighter,li2019federated,ma2017distributed}. These methods perform well in practice; however, the theoretical understanding is an open area for discovering.

Another idea to solve the communication problem in distributed algorithms is in randomly selecting some entries to update. Random selection is used to sparsify local gradients in the synchronous algorithm of \cite{wangni2018gradient}, to sparsify the variance-reducing term in the stochastic algorithm of \cite{leblond2016asaga} and \cite{pedregosa2017breaking}, or to sparsify updates in fixed-point iterations \cite{peng2016arock}. There are many different techniques of reducing communications that were investigated during the last years:  general quantization \cite{alistarh2017qsgd, horvath2019stochastic, koloskova2019decentralized}, ternary quantization \cite{wen2017terngrad}, $1$-bit quantization \cite{bernstein2018signsgd}, and others \cite{ben2019demystifying, lin2017deep}.


% {\color{red}  In \cite{mishchenko2019distributed} authors propose distributed gradient-type method with compression of gradient differences. The key idea is in the quantization of difference of the current stochastic gradient on each worker with an error term stored locally on each worker machine.} In the context of federated learning, \cite{konevcny2016federated} mentions the idea of random sparsification, but without further study.

Synchronous algorithms perform well when it takes approximately the same time for all machines to make their update. Otherwise, the slower worker machines may slow down the whole process as the faster ones have to wait for all updates in order to terminate their computation and exchange information. As a result, many approaches based on the asynchrony of communications have been recently proposed on distributed optimization methods without shared memory, see e.g.\;\cite{zhang2014asynchronous,ma2015adding,aytekin2016analysis,peng2016arock,calauzenes2017distributed}. In this case, worker machines update their parameters simultaneously on a local sub-part of data and send their updated parameters to a master machine which broadcasts the integrated updates back to each computing node for a new local update of their parameters \cite{li2013distributed,konevcny2016federated,ICML18}.


However, these methods generally suffer from communication cost between workers and the master machine and usually rely on restrictive assumptions on the computing system delays, which in turn impact the obtained convergence rates. To overcome these restrictions, asynchronous coordinate descent methods were recently proposed in \cite{hannah2016unbounded,sun2017asynchronous}. These techniques can handle unbounded delays, but they are based on decreasing stepsizes. In contrast, the recent works \cite{ICML18,mishchenko2018} provide a delay-independent analysis technique that allows integrating assumptions on the computing system with a constant stepsize.

