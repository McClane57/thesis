\section{Distributed learning}\label{sec:distributed-intro}
Given the tremendous production of data and the ever-growing size of collections, there is a surge of interest
% in both Machine Learning and Optimization communities 
for the development of efficient and scalable distributed learning strategies. \mitya{A distributed system could be interpreted as multiple entities (also mentioned as \emph{nodes}) that communicate in some way between each other, while also performing operations.} Training observations are generally split over different computing nodes, and learning is performed simultaneously. \mitya{This setup is different from shared-memory parallel computing, where each worker machine can potentially have access to all available memory \cite{Leslie91, Kumar02}}.%, and allows to process bigger datasets.}

\subsection{Computing setups}\label{sec:basics_dist-set}
All distributed algorithms could be segregated according to the network structure. Some of them use networks with one central node called \emph{master} that has a connection with all the other nodes called \emph{workers} \cite{konevcny2016federated,konevcny2016federated2,mishchenko2018}. In such setups usually, all the data is split between worker machines, and worker machines update their parameters simultaneously on a local sub-part of data and send their updated parameters to a master machine. The master integrates all received parameters and broadcasts them back to each computing node for a new local update of their parameters. This setup is called a \emph{centralized} (or \emph{master-worker}) setup. \dg{There} could be a different amount of layers: master communicates only with some of the other nodes that are the local masters and so on. Graphs of such networks are trees with the root being the master node, leaves being the workers, and all the others being the local masters.
% It is easy to see that from the theoretical point of view, such setup is quite the same as the first one.

There is another setup called decentralized, when all the nodes are the same and could communicate with neighbor nodes with respect to the network graph \cite{nedic2009distributed, boyd2011distributed, duchi2011dual, shi2015extra}. In this setup, the approach usually builds on the seminal work of Tsitsikas \cite{tsitsiklis1984problems} (see also \cite{bertsekas1997parallel} and \cite{tsitsiklis1986distributed}) who proposed a framework to analyze the distributed computation models.% Another important part is the exact computing averages of values that are initially stored on the nodes \cite{boyd2011distributed, olshevsky2006convergence,olshevsky2009convergence}. 

In our work, we focus on the centralized distributed setup without shared memory \cite{mishchenko2018} where all the data $\data$ is separated between $M$ workers and all local subsets $\data_i$ are stored on each machine. {We also consider the case when there is only one layer, and all worker machines are connected with the central node of the system - master node. }This setup could be further categorized into two approaches with different assumptions on the communication rounds - synchronous and asynchronous. In the synchronous approach, every communication round mobilizes all worker machines \cite{BoydPCPE11,Chen2016,Tsianos12}. \mitya{In asynchronous setting, the communication round does not require all workers to participate and could involve only one worker \cite{mishchenko2018}.}

\subsubsection{Synchronous algorithms}
The study of distributed SGD in a master-worker setting \cite{yang2013trading} raises the question of the communication-computation tradeoff. {Every communication round in a synchronous setting consists of two phases: \emph{All-Reduce synchronization step} when all the workers send their updates to the master and \emph{Broadcasting step} that shares the updated parameter vector with all workers. This could significantly increase the working time of optimization algorithms when the amount of communication rounds, workers, and bits in the update are big due to the communication overhead.} It promotes the paradigm of mini-batch methods \cite{dekel2012optimal, shalev2013accelerated, shamir2014distributed, qu2016coordinate, takavc2015distributed}. The main idea of these methods in generalizing stochastic methods to process multiple data points on every iteration to make communication rounds less often. However, when the significant reduction of communication required, the size of mini-batch becomes big, which revert stochastic methods to the full-batch methods. Another way to look at this idea is local SGD methods that have been investigated in \cite{stich2018local,khaled2019first,khaled2020tighter,li2019federated,ma2017distributed}. These methods perform well in practice; however, the theoretical understanding is an open area.

% \mitya{Recently, several approaches to mitigate this communication issue have been proposed,
% including different forms of gradient or model compression, which reduces representation size
% by quantization technique, when the idea is to reduce precision on elements (see e.g. general quantization \cite{alistarh2017qsgd, horvath2019stochastic, koloskova2019decentralized}, ternary quantization \cite{wen2017terngrad}, $1$-bit quantization \cite{bernstein2018signsgd}) or sparsification when only the most significant gradient entries are sent (see e.g. \cite{wang2017efficient, alistarh2018convergence, vogels2019powersgd}).}

Another idea to solve the communication problem in distributed algorithms is in randomly selecting some entries to update. Random selection is used to sparsify local gradients in the synchronous algorithm of \cite{wangni2018gradient}, to sparsify the variance-reducing term in the stochastic algorithm of \cite{leblond2016asaga} and \cite{pedregosa2017breaking}, or to sparsify updates in fixed-point iterations \cite{peng2016arock}. There are many different techniques of reducing communications that were investigated during the last years:  general quantization \cite{alistarh2017qsgd, horvath2019stochastic, koloskova2019decentralized}, ternary quantization \cite{wen2017terngrad}, $1$-bit quantization \cite{bernstein2018signsgd}, and others \cite{ben2019demystifying, lin2017deep}.


% {\color{red}  In \cite{mishchenko2019distributed} authors propose distributed gradient-type method with compression of gradient differences. The key idea is in the quantization of difference of the current stochastic gradient on each worker with an error term stored locally on each worker machine.} In the context of federated learning, \cite{konevcny2016federated} mentions the idea of random sparsification, but without further study.

\subsubsection{Asynchronous algorithms}
Synchronous algorithms perform well when it takes approximately the same time for all machines to make their update. Otherwise, the slower worker machines may slow down the whole process as the faster ones have to wait for all updates in order to terminate their computation and exchange information. As a result, many approaches based on the asynchrony of communications have been recently proposed on distributed optimization methods without shared memory, see e.g., \;\cite{zhang2014asynchronous,ma2015adding,aytekin2016analysis,peng2016arock,calauzenes2017distributed}. In this case, worker machines update their parameters simultaneously on a local sub-part of data and send their updated parameters to a master machine which broadcasts the integrated updates back to each computing node for a new local update of their parameters \cite{li2013distributed,konevcny2016federated,ICML18}.


However, these methods generally suffer from communication cost between workers and the master machine and usually rely on restrictive assumptions on the computing system delays, which in turn impact the obtained convergence rates. To overcome these restrictions, asynchronous coordinate descent methods were recently proposed in \cite{hannah2016unbounded,sun2017asynchronous}. These techniques can handle unbounded delays, but they are based on decreasing stepsizes. In contrast, the recent works \cite{ICML18,mishchenko2018} provide a delay-independent analysis technique that allows integrating assumptions on the computing system with a constant stepsize.

