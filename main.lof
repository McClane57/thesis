\addvspace {10\p@ }
\contentsline {figure}{\numberline {1-1}{\ignorespaces Graphical illustration of lower and upper bound for $L$-smooth and $\mu $-strongly convex function $f:\mathbb {R}\rightarrow \mathbb {R}$\relax }}{5}{figure.caption.4}% 
\contentsline {figure}{\numberline {1-2}{\ignorespaces Linear approximations of non-smooth function $r = \qopname \relax m{max}\left \{-2x, (0.2x+1)^2 - 1\right \}$ from $\mathbb {R}$ to $\mathbb {R}$ at point $y = 0$ with different subgradients. As we could see, $y=0$ is a minimizer of $r$, however its subdifferential in this point is $\partial _0 r = [-2, 0.4]\ni 0$. Depending on the subgradient $g$ (we plot for $g=-1, -0.5, 0.4$) different linear approximations of function $r$ in $0$ appears and, as a result, the ``descent'' step is the descent step for these approximations but not for $r$.\relax }}{9}{figure.caption.5}% 
\contentsline {figure}{\numberline {1-3}{\ignorespaces Geometrical interpretation of soft-thresholding operator\relax }}{11}{figure.caption.7}% 
\contentsline {figure}{\numberline {1-4}{\ignorespaces Geometrical interpretation of Moreau envelope for $r = \qopname \relax m{max}\left \{-x, 0.5x\right \}$ from $\mathbb {R}$ to $\mathbb {R}$.\relax }}{12}{figure.caption.8}% 
\contentsline {figure}{\numberline {1-5}{\ignorespaces Evolution of iterates for GD with fixed stepsize $\gamma = \frac {2}{\mu + L}$ and SGD with decreasing stepsize $\gamma ^k = \frac {1}{k}$ for $L$-smooth and $\mu $-strongly convex objective $f(x) = \delimiter "026B30D Ax-b\delimiter "026B30D _2^2$, with random generated $A\in \mathbb {R}^{100\times 100},b\in \mathbb {R}^{100}$. As we could see from this plot, in the beginning, when the stepsizes in these two algorithm are approximately the same SGD performs better, however when $\gamma ^k\ll \frac {1}{L}$ it slows down fast.\relax }}{16}{figure.caption.12}% 
\contentsline {figure}{\numberline {1-6}{\ignorespaces $\ell _1$ identification\relax }}{21}{figure.caption.13}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2-1}{\ignorespaces Summary of notations about iteration, adaptation and filtration. The filtration $\mathcal {F}^{k-1}$ is the sigma-algebra generated by $\{\mathfrak {S}^\ell \}_{\ell \leq k-1}$ encompassing the knowledge of all variables up to $y^k$ (but not $z^k$).\relax }}{33}{figure.caption.17}% 
\contentsline {figure}{\numberline {2-2}{\ignorespaces Comparisons between theoretical and harsh updating time for \texttt {ARPSD}.\relax }}{35}{figure.caption.18}% 
\contentsline {figure}{\numberline {2-3}{\ignorespaces $\ell _1$-regularized logistic regression \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:logl1}\unskip \@@italiccorr )}} \relax }}{43}{figure.caption.26}% 
\contentsline {figure}{\numberline {2-4}{\ignorespaces $\ell _{1,2}$ regularized logistic regression \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:logl12}\unskip \@@italiccorr )}} \relax }}{44}{figure.caption.28}% 
\contentsline {figure}{\numberline {2-5}{\ignorespaces 1D-TV-regularized logistic regression \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:logtv}\unskip \@@italiccorr )}}\relax }}{45}{figure.caption.29}% 
\contentsline {figure}{\numberline {2-6}{\ignorespaces 20 runs of \texttt {ARPSD}\nobreakspace {}and their median (in bold) on 1D-TV-regularized logistic regression \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:logtv}\unskip \@@italiccorr )}} \relax }}{46}{figure.caption.30}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3-1}{\ignorespaces Notations of delays at iteration $k$.\relax }}{49}{figure.caption.31}% 
