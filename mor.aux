\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Automatic dimension reduction}{48}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:MOR}{{2}{48}{Automatic dimension reduction}{chapter.2}{}}
\@writefile{toc}{\etoc@startlocaltoc{3}}
\citation{richtarik2014iteration}
\citation{necoara2014random}
\citation{zhao2015stochastic}
\citation{richtarik2016optimal}
\citation{loshchilov2011adaptive}
\citation{glasmachers2013accelerated}
\citation{dhillon2011nearest}
\citation{nutini2015coordinate}
\citation{nutini2017let}
\citation{perekrestenko2017faster}
\citation{namkoong2017adaptive}
\citation{stich2017safe}
\newlabel{sec:mor-intro}{{2}{49}{Introduction}{section*.29}{}}
\@writefile{toc}{\contentsline {section}{Introduction}{49}{section*.29}\protected@file@percent }
\newlabel{eq:main_problem}{{2.1}{49}{Introduction}{equation.2.0.1}{}}
\citation{grishchenko2020proximal}
\@writefile{toc}{\contentsline {paragraph}{Outline.}{50}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Randomized subspace descent}{50}{section.2.1}\protected@file@percent }
\newlabel{sec:mor-randomized-subspace}{{2.1}{50}{Randomized subspace descent}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Subspace selection}{50}{subsection.2.1.1}\protected@file@percent }
\newlabel{sec:sub}{{2.1.1}{50}{Subspace selection}{subsection.2.1.1}{}}
\newlabel{def:cov}{{2.1}{51}{Covering family of subspaces}{theorem.2.1}{}}
\newlabel{ex:axes}{{2.2}{51}{}{theorem.2.2}{}}
\newlabel{lm:eligible}{{2.4}{52}{Average projection}{theorem.2.4}{}}
\newlabel{eq:pbar}{{2.2}{52}{Average projection}{equation.2.1.2}{}}
\newlabel{rem:selection}{{2.5}{52}{Finite Subspace Families}{theorem.2.5}{}}
\citation{qu2016coordinate}
\citation{hanzely2018sega}
\citation{mishchenko2018}
\newlabel{ex:P}{{2.6}{53}{Coordinate-wise projections}{theorem.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Random Subspace Proximal Gradient Algorithm}{53}{subsection.2.1.2}\protected@file@percent }
\newlabel{sec:mor-algo}{{2.1.2}{53}{Random Subspace Proximal Gradient Algorithm}{subsection.2.1.2}{}}
\citation{hanzely2018sega}
\newlabel{eq:grad_step}{{2.3}{54}{Random Subspace Proximal Gradient Algorithm}{equation.2.1.3}{}}
\newlabel{eq:prox_step}{{2.4}{54}{Random Subspace Proximal Gradient Algorithm}{equation.2.1.4}{}}
\citation{frongillo2015convergence}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces Randomized Proximal Subspace Descent - \texttt  {RPSD}\relax }}{55}{algorithm.9}\protected@file@percent }
\newlabel{alg:strata_nondis}{{9}{55}{Randomized Proximal Subspace Descent - \algo \relax }{algorithm.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Analysis and convergence rate}{55}{subsection.2.1.3}\protected@file@percent }
\newlabel{sec:conv}{{2.1.3}{55}{Analysis and convergence rate}{subsection.2.1.3}{}}
\newlabel{hyp:f}{{2.8}{55}{On the optimization problem}{theorem.2.8}{}}
\newlabel{hyp:main}{{2.9}{56}{On the randomness of the algorithm}{theorem.2.9}{}}
\newlabel{th:conv_nondis}{{2.10}{56}{\algo ~convergence rate}{theorem.2.10}{}}
\newlabel{lm:removing_exp}{{2.11}{56}{Expression of the decrease as a martingale}{theorem.2.11}{}}
\newlabel{lm:bub}{{2.12}{57}{Contraction property in $\bP $-weighted norm}{theorem.2.12}{}}
\citation{bauschke2011convex}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Examples and connections with the existing work}{59}{subsection.2.1.4}\protected@file@percent }
\newlabel{sec:comparison}{{2.1.4}{59}{Examples and connections with the existing work}{subsection.2.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Projections onto coordinates}{59}{section*.31}\protected@file@percent }
\newlabel{sec:coordproj}{{2.1.4}{59}{Projections onto coordinates}{section*.31}{}}
\citation{tseng2001convergence}
\citation{nesterov2012efficiency}
\citation{richtarik2014iteration}
\citation{wright2015coordinate}
\citation{richtarik2014iteration}
\citation{tibshirani2005sparsity}
\@writefile{toc}{\contentsline {subsubsection}{Projections onto vectors of fixed variations}{60}{section*.32}\protected@file@percent }
\newlabel{sec:var}{{2.1.4}{60}{Projections onto vectors of fixed variations}{section*.32}{}}
\newlabel{eq:TV}{{2.5}{60}{Projections onto vectors of fixed variations}{equation.2.1.5}{}}
\citation{hanzely2018sega}
\newlabel{eq:proj_tv}{{2.6}{61}{Projections onto vectors of fixed variations}{equation.2.1.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison with sketching}{61}{section*.33}\protected@file@percent }
\citation{richtarik2014iteration}
\citation{necoara2014random}
\citation{dhillon2011nearest}
\citation{nutini2015coordinate}
\citation{nutini2017let}
\citation{perekrestenko2017faster}
\citation{namkoong2017adaptive}
\citation{stich2017safe}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Adaptive subspace descent}{62}{section.2.2}\protected@file@percent }
\newlabel{sec:mor-adaptive-subspace}{{2.2}{62}{Adaptive subspace descent}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Random Subspace Descent with time-varying selection}{62}{subsection.2.2.1}\protected@file@percent }
\newlabel{sec:ada_algo}{{2.2.1}{62}{Random Subspace Descent with time-varying selection}{subsection.2.2.1}{}}
\newlabel{hyp:main_identif}{{2.13}{63}{On the randomness of the adaptive algorithm}{theorem.2.13}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10}{\ignorespaces Adaptive Randomized Proximal Subspace Descent - \texttt  {ARPSD}\relax }}{63}{algorithm.10}\protected@file@percent }
\newlabel{alg:ada_strata_nondis}{{10}{63}{Adaptive Randomized Proximal Subspace Descent - \adaalgo \relax }{algorithm.10}{}}
\newlabel{line:rescale}{{10}{63}{Adaptive Randomized Proximal Subspace Descent - \adaalgo \relax }{ALC@unique.51}{}}
\newlabel{th:conv_nondis_arbitrary}{{2.14}{63}{\adaalgo ~convergence}{theorem.2.14}{}}
\newlabel{eq:corr-rate}{{2.7}{64}{\adaalgo ~convergence}{equation.2.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-1}{\ignorespaces Summary of notations about iteration, adaptation and filtration. The filtration $\mathcal  {F}^{k-1}$ is the sigma-algebra generated by $\{\mathfrak  {S}^\ell \}_{\ell \leq k-1}$ encompassing the knowledge of all variables up to $y^k$ (but not $z^k$).\relax }}{64}{figure.caption.34}\protected@file@percent }
\newlabel{fig:proof}{{2-1}{64}{Summary of notations about iteration, adaptation and filtration. The filtration $\mathcal {F}^{k-1}$ is the sigma-algebra generated by $\{\Sel ^\ell \}_{\ell \leq k-1}$ encompassing the knowledge of all variables up to $y^k$ (but not $z^k$).\relax }{figure.caption.34}{}}
\newlabel{eq:iterate_flexible_lambda}{{2.8}{65}{Random Subspace Descent with time-varying selection}{equation.2.2.8}{}}
\newlabel{eq:iterate_flexible_lambda2}{{2.9}{65}{Random Subspace Descent with time-varying selection}{equation.2.2.9}{}}
\newlabel{eq:Jadd}{{2.10}{65}{Random Subspace Descent with time-varying selection}{equation.2.2.10}{}}
\newlabel{eq:adapt}{{2.11}{65}{Random Subspace Descent with time-varying selection}{equation.2.2.11}{}}
\newlabel{ex:adapt_fixed}{{2.15}{66}{Explicit convergence rate}{theorem.2.15}{}}
\newlabel{eq:explicit}{{2.12}{66}{Explicit convergence rate}{equation.2.2.12}{}}
\newlabel{eq:min_adapt}{{2.13}{66}{Explicit convergence rate}{equation.2.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-2}{\ignorespaces Comparisons between theoretical and harsh updating time for \texttt  {ARPSD}.\relax }}{67}{figure.caption.35}\protected@file@percent }
\newlabel{fig:stab}{{2-2}{67}{Comparisons between theoretical and harsh updating time for \adaalgo .\relax }{figure.caption.35}{}}
\newlabel{th:aggressive}{{2.17}{68}{\adaalgo ~convergence: practical version}{theorem.2.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Identification of proximal algorithms}{69}{subsection.2.2.2}\protected@file@percent }
\newlabel{sec:identif}{{2.2.2}{69}{Identification of proximal algorithms}{subsection.2.2.2}{}}
\newlabel{th:rate_identif}{{2.18}{70}{Improved asymptotic rate}{theorem.2.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Identification-based subspace descent}{71}{subsection.2.2.3}\protected@file@percent }
\newlabel{sec:adapt}{{2.2.3}{71}{Identification-based subspace descent}{subsection.2.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{How to update the selection}{71}{section*.36}\protected@file@percent }
\newlabel{sec:howto}{{2.2.3}{71}{How to update the selection}{section*.36}{}}
\newlabel{ex:supp}{{2.20}{71}{{Complemented subspaces and sparsity vectors} for axes and jumps}{theorem.2.20}{}}
\newlabel{eq:Ccoord}{{2.14}{71}{{Complemented subspaces and sparsity vectors} for axes and jumps}{equation.2.2.14}{}}
\newlabel{eq:Mcoord}{{2.15}{71}{{Complemented subspaces and sparsity vectors} for axes and jumps}{equation.2.2.15}{}}
\newlabel{eq:Cjump}{{2.16}{72}{{Complemented subspaces and sparsity vectors} for axes and jumps}{equation.2.2.16}{}}
\newlabel{eq:Mjump}{{2.17}{72}{{Complemented subspaces and sparsity vectors} for axes and jumps}{equation.2.2.17}{}}
\citation{stich2017safe}
\citation{grishchenko2018asynchronous}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Strategies for non-adaptive vs.\tmspace  +\thickmuskip {.2777em}adaptive algorithms\relax }}{73}{table.caption.37}\protected@file@percent }
\newlabel{tab:comp}{{2.1}{73}{Strategies for non-adaptive vs.\;adaptive algorithms\relax }{table.caption.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{Practical examples and discussion}{73}{section*.38}\protected@file@percent }
\newlabel{sec:ex_ada}{{2.2.3}{73}{Practical examples and discussion}{section*.38}{}}
\newlabel{Pex:l1}{{2.2.3}{73}{Coordinate-wise projections}{section*.39}{}}
\@writefile{toc}{\contentsline {paragraph}{Coordinate-wise projections}{73}{section*.39}\protected@file@percent }
\newlabel{Pex:TV}{{2.2.3}{75}{Vectors of fixed variations}{section*.40}{}}
\@writefile{toc}{\contentsline {paragraph}{Vectors of fixed variations}{75}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Practical consideration for TV}{75}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Numerical illustrations}{76}{section.2.3}\protected@file@percent }
\newlabel{sec:mor-numerical}{{2.3}{76}{Numerical illustrations}{section.2.3}{}}
\citation{nesterov2012efficiency}
\citation{hanzely2018sega}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Experimental setup}{77}{subsection.2.3.1}\protected@file@percent }
\newlabel{eq:logl1}{{2.18a}{77}{Experimental setup}{equation.2.3.1}{}}
\newlabel{eq:logl12}{{2.18b}{77}{Experimental setup}{equation.2.3.2}{}}
\newlabel{eq:logtv}{{2.18c}{77}{Experimental setup}{equation.2.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Illustrations for coordinate-structured problems}{78}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparison with standard methods}{78}{section*.42}\protected@file@percent }
\citation{hanzely2018sega}
\citation{hanzely2018sega}
\@writefile{lof}{\contentsline {figure}{\numberline {2-3}{\ignorespaces $\ell _1$-regularized logistic regression \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:logl1}\unskip \@@italiccorr )}} \relax }}{79}{figure.caption.43}\protected@file@percent }
\newlabel{fig:rcv1}{{2-3}{79}{$\ell _1$-regularized logistic regression \eqref {eq:logl1} \relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison with \texttt  {SEGA}}{79}{section*.44}\protected@file@percent }
\newlabel{sec:num:sega}{{2.3.2}{79}{Comparison with \sega }{section*.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-4}{\ignorespaces $\ell _{1,2}$ regularized logistic regression \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:logl12}\unskip \@@italiccorr )}} \relax }}{80}{figure.caption.45}\protected@file@percent }
\newlabel{fig:rcv1_l12}{{2-4}{80}{$\ell _{1,2}$ regularized logistic regression \eqref {eq:logl12} \relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Illustrations for total variation regularization}{80}{subsection.2.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2-5}{\ignorespaces 1D-TV-regularized logistic regression \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:logtv}\unskip \@@italiccorr )}}\relax }}{81}{figure.caption.46}\protected@file@percent }
\newlabel{fig:a11}{{2-5}{81}{1D-TV-regularized logistic regression \eqref {eq:logtv}\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-6}{\ignorespaces 20 runs of \texttt  {ARPSD}\nobreakspace  {}and their median (in bold) on 1D-TV-regularized logistic regression \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:logtv}\unskip \@@italiccorr )}} \relax }}{82}{figure.caption.47}\protected@file@percent }
\newlabel{fig:tvcomp}{{2-6}{82}{20 runs of \adaalgo ~and their median (in bold) on 1D-TV-regularized logistic regression \eqref {eq:logtv} \relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Conclusion}{82}{section.2.4}\protected@file@percent }
\@setckpt{mor}{
\setcounter{page}{83}
\setcounter{equation}{18}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{4}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{1}
\setcounter{savepage}{0}
\setcounter{AM@survey}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{8}
\setcounter{bookmark@seq@number}{32}
\setcounter{parentequation}{18}
\setcounter{etoc@tocid}{3}
\setcounter{etoc@tocdepth}{2}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{float@type}{16}
\setcounter{pp@next@reset}{0}
\setcounter{algorithm}{10}
\setcounter{ALC@unique}{53}
\setcounter{ALC@line}{12}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{0}
\setcounter{algocfproc}{0}
\setcounter{algocf}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{theorem}{20}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{section@level}{1}
}
