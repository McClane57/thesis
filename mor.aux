\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{candes2008enhancing}
\citation{combettes2011proximal}
\citation{bach2012optimization}
\citation{teboulle2018simplified}
\citation{wright2015coordinate}
\citation{richtarik2014iteration}
\citation{necoara2014random}
\citation{zhao2015stochastic}
\citation{richtarik2016optimal}
\citation{loshchilov2011adaptive}
\citation{glasmachers2013accelerated}
\citation{dhillon2011nearest}
\citation{nutini2015coordinate}
\citation{nutini2017let}
\citation{perekrestenko2017faster}
\citation{namkoong2017adaptive}
\citation{stich2017safe}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Automatic dimension reduction}{23}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:MOR}{{2}{23}{Automatic dimension reduction}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{23}{section.2.1}\protected@file@percent }
\newlabel{sec:mor-intro}{{2.1}{23}{Introduction}{section.2.1}{}}
\newlabel{eq:main_problem}{{2.1}{23}{Introduction}{equation.2.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Randomized subspace descent}{24}{section.2.2}\protected@file@percent }
\newlabel{sec:mor-randomized-subspace}{{2.2}{24}{Randomized subspace descent}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Subspace selection}{24}{subsection.2.2.1}\protected@file@percent }
\newlabel{sec:sub}{{2.2.1}{24}{Subspace selection}{subsection.2.2.1}{}}
\newlabel{def:cov}{{2.1}{24}{Covering family of subspaces}{theorem.2.1}{}}
\newlabel{ex:axes}{{2.2}{24}{}{theorem.2.2}{}}
\newlabel{lm:eligible}{{2.4}{25}{Average projection}{theorem.2.4}{}}
\newlabel{eq:pbar}{{2.2}{25}{Average projection}{equation.2.2.2}{}}
\newlabel{rem:selection}{{2.5}{25}{Finite Subspace Families}{theorem.2.5}{}}
\newlabel{ex:P}{{2.6}{25}{Coordinate-wise projections}{theorem.2.6}{}}
\citation{bauschke2011convex}
\citation{combettes2008proximal}
\citation{yuan2011efficient}
\citation{condat2013direct}
\citation{qu2016coordinate}
\citation{hanzely2018sega}
\citation{mishchenko2018}
\citation{hanzely2018sega}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Random Subspace Proximal Gradient Algorithm}{26}{subsection.2.2.2}\protected@file@percent }
\newlabel{sec:algo}{{2.2.2}{26}{Random Subspace Proximal Gradient Algorithm}{subsection.2.2.2}{}}
\newlabel{eq:pgGradProx}{{2.3}{26}{Random Subspace Proximal Gradient Algorithm}{equation.2.2.3}{}}
\newlabel{eq:pgGrad}{{2.3a}{26}{Random Subspace Proximal Gradient Algorithm}{equation.2.2.1}{}}
\newlabel{eq:pgProx}{{2.3b}{26}{Random Subspace Proximal Gradient Algorithm}{equation.2.2.2}{}}
\newlabel{eq:grad_step}{{2.4}{26}{Random Subspace Proximal Gradient Algorithm}{equation.2.2.4}{}}
\newlabel{eq:prox_step}{{2.5}{26}{Random Subspace Proximal Gradient Algorithm}{equation.2.2.5}{}}
\citation{frongillo2015convergence}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces Randomized Proximal Subspace Descent - \texttt  {RPSD}\relax }}{27}{algorithm.8}\protected@file@percent }
\newlabel{alg:strata_nondis}{{8}{27}{Randomized Proximal Subspace Descent - \algo \relax }{algorithm.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Analysis and convergence rate}{27}{subsection.2.2.3}\protected@file@percent }
\newlabel{sec:conv}{{2.2.3}{27}{Analysis and convergence rate}{subsection.2.2.3}{}}
\newlabel{hyp:f}{{2.8}{27}{On the optimization problem}{theorem.2.8}{}}
\newlabel{hyp:main}{{2.9}{27}{On the randomness of the algorithm}{theorem.2.9}{}}
\newlabel{th:conv_nondis}{{2.10}{27}{\algo ~convergence rate}{theorem.2.10}{}}
\newlabel{lm:removing_exp}{{2.11}{28}{Expression of the decrease as a martingale}{theorem.2.11}{}}
\newlabel{lm:bub}{{2.12}{28}{Contraction property in $\bP $-weighted norm}{theorem.2.12}{}}
\citation{bauschke2011convex}
\citation{tseng2001convergence}
\citation{nesterov2012efficiency}
\citation{richtarik2014iteration}
\citation{wright2015coordinate}
\citation{richtarik2014iteration}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Examples and Connections with the existing work}{30}{subsection.2.2.4}\protected@file@percent }
\newlabel{sec:comparison}{{2.2.4}{30}{Examples and Connections with the existing work}{subsection.2.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Projections onto coordinates}{30}{section*.14}\protected@file@percent }
\newlabel{sec:coordproj}{{2.2.4}{30}{Projections onto coordinates}{section*.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{Projections onto vectors of fixed variations}{30}{section*.15}\protected@file@percent }
\newlabel{sec:var}{{2.2.4}{30}{Projections onto vectors of fixed variations}{section*.15}{}}
\citation{tibshirani2005sparsity}
\citation{hanzely2018sega}
\newlabel{eq:TV}{{2.6}{31}{Projections onto vectors of fixed variations}{equation.2.2.6}{}}
\newlabel{eq:proj_tv}{{2.7}{31}{Projections onto vectors of fixed variations}{equation.2.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison with sketching}{31}{section*.16}\protected@file@percent }
\citation{richtarik2014iteration}
\citation{necoara2014random}
\citation{dhillon2011nearest}
\citation{nutini2015coordinate}
\citation{nutini2017let}
\citation{perekrestenko2017faster}
\citation{namkoong2017adaptive}
\citation{stich2017safe}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Adaptive subspace descent}{32}{section.2.3}\protected@file@percent }
\newlabel{sec:mor-adaptive-subspace}{{2.3}{32}{Adaptive subspace descent}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Random Subspace Descent with Time-Varying Selection}{32}{subsection.2.3.1}\protected@file@percent }
\newlabel{sec:ada_algo}{{2.3.1}{32}{Random Subspace Descent with Time-Varying Selection}{subsection.2.3.1}{}}
\newlabel{hyp:main_identif}{{2.13}{32}{On the randomness of the adaptive algorithm}{theorem.2.13}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces Adaptive Randomized Proximal Subspace Descent - \texttt  {ARPSD}\relax }}{33}{algorithm.9}\protected@file@percent }
\newlabel{alg:ada_strata_nondis}{{9}{33}{Adaptive Randomized Proximal Subspace Descent - \adaalgo \relax }{algorithm.9}{}}
\newlabel{line:rescale}{{10}{33}{Adaptive Randomized Proximal Subspace Descent - \adaalgo \relax }{ALC@unique.51}{}}
\newlabel{th:conv_nondis_arbitrary}{{2.14}{33}{\adaalgo ~convergence}{theorem.2.14}{}}
\newlabel{eq:corr-rate}{{2.8}{33}{\adaalgo ~convergence}{equation.2.3.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-1}{\ignorespaces Summary of notations about iteration, adaptation and filtration. The filtration $\mathcal  {F}^{k-1}$ is the sigma-algebra generated by $\{\mathfrak  {S}^\ell \}_{\ell \leq k-1}$ encompassing the knowledge of all variables up to $y^k$ (but not $z^k$).\relax }}{33}{figure.caption.17}\protected@file@percent }
\newlabel{fig:proof}{{2-1}{33}{Summary of notations about iteration, adaptation and filtration. The filtration $\mathcal {F}^{k-1}$ is the sigma-algebra generated by $\{\Sel ^\ell \}_{\ell \leq k-1}$ encompassing the knowledge of all variables up to $y^k$ (but not $z^k$).\relax }{figure.caption.17}{}}
\newlabel{eq:iterate_flexible_lambda}{{2.9}{34}{Random Subspace Descent with Time-Varying Selection}{equation.2.3.9}{}}
\newlabel{eq:iterate_flexible_lambda2}{{2.10}{34}{Random Subspace Descent with Time-Varying Selection}{equation.2.3.10}{}}
\newlabel{eq:Jadd}{{2.11}{34}{Random Subspace Descent with Time-Varying Selection}{equation.2.3.11}{}}
\newlabel{eq:adapt}{{2.12}{34}{Random Subspace Descent with Time-Varying Selection}{equation.2.3.12}{}}
\newlabel{ex:adapt_fixed}{{2.15}{34}{Explicit convergence rate}{theorem.2.15}{}}
\newlabel{eq:explicit}{{2.13}{35}{Explicit convergence rate}{equation.2.3.13}{}}
\newlabel{eq:min_adapt}{{2.14}{35}{Explicit convergence rate}{equation.2.3.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-2}{\ignorespaces Comparisons between theoretical and harsh updating time for \texttt  {ARPSD}.\relax }}{35}{figure.caption.18}\protected@file@percent }
\newlabel{fig:stab}{{2-2}{35}{Comparisons between theoretical and harsh updating time for \adaalgo .\relax }{figure.caption.18}{}}
\newlabel{th:aggressive}{{2.17}{36}{\adaalgo ~convergence: practical version}{theorem.2.17}{}}
\citation{lewis2002active}
\citation{vaiter2015model}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Identification of proximal algorithms}{37}{subsection.2.3.2}\protected@file@percent }
\newlabel{sec:identif}{{2.3.2}{37}{Identification of proximal algorithms}{subsection.2.3.2}{}}
\newlabel{th:rate_identif}{{2.18}{37}{Improved asymptotic rate}{theorem.2.18}{}}
\newlabel{eq:qualif}{{{QC}}{37}{Improved asymptotic rate}{theorem.2.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Identification-based Subspace Descent}{38}{subsection.2.3.3}\protected@file@percent }
\newlabel{sec:adapt}{{2.3.3}{38}{Identification-based Subspace Descent}{subsection.2.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{How to update the selection}{38}{section*.19}\protected@file@percent }
\newlabel{sec:howto}{{2.3.3}{38}{How to update the selection}{section*.19}{}}
\newlabel{ex:supp}{{2.20}{38}{{Complemented subspaces and sparsity vectors} for axes and jumps}{theorem.2.20}{}}
\newlabel{eq:Ccoord}{{2.15}{38}{{Complemented subspaces and sparsity vectors} for axes and jumps}{equation.2.3.15}{}}
\newlabel{eq:Mcoord}{{2.16}{38}{{Complemented subspaces and sparsity vectors} for axes and jumps}{equation.2.3.16}{}}
\newlabel{eq:Cjump}{{2.17}{38}{{Complemented subspaces and sparsity vectors} for axes and jumps}{equation.2.3.17}{}}
\newlabel{eq:Mjump}{{2.18}{38}{{Complemented subspaces and sparsity vectors} for axes and jumps}{equation.2.3.18}{}}
\citation{stich2017safe}
\citation{grishchenko2018asynchronous}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Strategies for non-adaptive vs.\tmspace  +\thickmuskip {.2777em}adaptive algorithms\relax }}{39}{table.caption.20}\protected@file@percent }
\newlabel{tab:comp}{{2.1}{39}{Strategies for non-adaptive vs.\;adaptive algorithms\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{Practical Examples and Discussion}{39}{section*.21}\protected@file@percent }
\newlabel{sec:ex_ada}{{2.3.3}{39}{Practical Examples and Discussion}{section*.21}{}}
\newlabel{Pex:l1}{{2.3.3}{39}{Coordinate-wise projections}{section*.22}{}}
\@writefile{toc}{\contentsline {paragraph}{Coordinate-wise projections}{39}{section*.22}\protected@file@percent }
\newlabel{Pex:TV}{{2.3.3}{40}{Vectors of fixed variations}{section*.23}{}}
\@writefile{toc}{\contentsline {paragraph}{Vectors of fixed variations}{40}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Practical consideration for TV}{41}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Numerical illustrations}{41}{section.2.4}\protected@file@percent }
\newlabel{sec:mor-numerical}{{2.4}{41}{Numerical illustrations}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Experimental setup}{41}{subsection.2.4.1}\protected@file@percent }
\newlabel{eq:logl1}{{2.19a}{41}{Experimental setup}{equation.2.4.1}{}}
\newlabel{eq:logl12}{{2.19b}{41}{Experimental setup}{equation.2.4.2}{}}
\newlabel{eq:logtv}{{2.19c}{41}{Experimental setup}{equation.2.4.3}{}}
\citation{nesterov2012efficiency}
\citation{hanzely2018sega}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Illustrations for coordinate-structured problems}{42}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparison with standard methods}{42}{section*.25}\protected@file@percent }
\citation{hanzely2018sega}
\citation{hanzely2018sega}
\@writefile{lof}{\contentsline {figure}{\numberline {2-3}{\ignorespaces $\ell _1$-regularized logistic regression \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:logl1}\unskip \@@italiccorr )}} \relax }}{43}{figure.caption.26}\protected@file@percent }
\newlabel{fig:rcv1}{{2-3}{43}{$\ell _1$-regularized logistic regression \eqref {eq:logl1} \relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison with \texttt  {SEGA}}{43}{section*.27}\protected@file@percent }
\newlabel{sec:num:sega}{{2.4.2}{43}{Comparison with \sega }{section*.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-4}{\ignorespaces $\ell _{1,2}$ regularized logistic regression \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:logl12}\unskip \@@italiccorr )}} \relax }}{44}{figure.caption.28}\protected@file@percent }
\newlabel{fig:rcv1_l12}{{2-4}{44}{$\ell _{1,2}$ regularized logistic regression \eqref {eq:logl12} \relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Illustrations for total variation regularization}{44}{subsection.2.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2-5}{\ignorespaces 1D-TV-regularized logistic regression \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:logtv}\unskip \@@italiccorr )}}\relax }}{45}{figure.caption.29}\protected@file@percent }
\newlabel{fig:a11}{{2-5}{45}{1D-TV-regularized logistic regression \eqref {eq:logtv}\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2-6}{\ignorespaces 20 runs of \texttt  {ARPSD}\nobreakspace  {}and their median (in bold) on 1D-TV-regularized logistic regression \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:logtv}\unskip \@@italiccorr )}} \relax }}{46}{figure.caption.30}\protected@file@percent }
\newlabel{fig:tvcomp}{{2-6}{46}{20 runs of \adaalgo ~and their median (in bold) on 1D-TV-regularized logistic regression \eqref {eq:logtv} \relax }{figure.caption.30}{}}
\@setckpt{mor}{
\setcounter{page}{47}
\setcounter{equation}{19}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{5}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{1}
\setcounter{savepage}{0}
\setcounter{linenumber}{320}
\setcounter{LN@truepage}{49}
\setcounter{AM@survey}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{7}
\setcounter{bookmark@seq@number}{24}
\setcounter{parentequation}{19}
\setcounter{caption@flags}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{float@type}{16}
\setcounter{pp@next@reset}{0}
\setcounter{algorithm}{9}
\setcounter{ALC@unique}{53}
\setcounter{ALC@line}{12}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{0}
\setcounter{algocfproc}{0}
\setcounter{algocf}{0}
\setcounter{tcbbreakpart}{0}
\setcounter{tcblayer}{0}
\setcounter{theorem}{20}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{section@level}{2}
}
