In the big data era, both the amount of examples $m$ and the dimension of features $n$ can be very large, which excludes higher-order optimization methods. Under this setting, the computational cost of each gradient could be large because it requires passing through all the $m$ training points. So the gradient and coordinate (if $m\gg n$)  methods that we have presented so far are also expensive. This calls for incremental gradient methods that have the cost of each iteration that does not depend on $m$. Let us recall that in machine learning, the usual problem is ERM \eqref{eq:erm}, where the objective function $f$ has a sum structure (probably infinite).

% For example, the full gradient computation is impossible in the problems that appear in statistical learning applications, when the objective function is the infinite sum
% $$
% f(x) = \EE_{\eta\sim\mathcal{D}}f_{\eta}(x),
% $$
% where distribution $\mathcal{D}$ is not known but samples $\eta\sim\mathcal{D}$ are available. It means that $\nabla f(x)$ is impossible to compute but every $\nabla f_{\eta}(x)$ are computable and moreover, $\nabla f_{\eta}(x)$ is an unbiased estimator of the gradient of $f$
% $$
% \EE_{\eta\sim\mathcal{D}}\nabla f_{\eta}(x) = \nabla f(x).
% $$
In case of finite sum
$$
f(x) = \frac{1}{m}\sum_{i=1}^m f_i(x)
$$
if $i$ is selected uniformly at random from $[m] = [1,\dots,m]$ then $\nabla f_i(x)$ is also an unbiased estimator of gradient of $f$, but its computation is about $m$ times faster than computation of $\nabla f(x)$

Let us present the general scheme of stochastic gradient descent
\begin{algorithm}
    \caption{Stochastic Gradient Descent (SGD)}
    \label{algo:sgd}
    \begin{algorithmic}
        \STATE Initialize $x^0\in\mathbb{R}^n$
        \FOR{$k\geq 0$}{
            \STATE Compute an unbiased estimator $g^k$\hfill $\EE\left[g^k|x^k\right] = \nabla f(x^k)$
            \STATE $x^{k+1}\leftarrow x^k - \gamma^k g^k$
        }
        \ENDFOR
    \end{algorithmic}
\end{algorithm}
where by $\EE\left[g^k|x^k\right]$ we denote conditional expectation \cite[Chapter IV, Section $4$]{kolmogorov1933grundbegriffe}.
In particular, $g^k$ could be selected as $\nabla f_{i^k}(x^k)$ where $i^k$ is an index uniformly generated among $1,\ldots, m$. The vector $g^k$ is a noisy approximation of the real gradient $\nabla f(x^k)$. The difference $g^k- \nabla f(x^k)$ represents this noise and its variance is the important measure of quality
\begin{equation}\label{eq:sg_variance}
    \EE\left[\|g^k - \nabla f(x^k)\|_2^2|x^k\right].
\end{equation}
This variance is usually non-zero for any $x^k$ and even in the optimal point $x^\star$. For example, the full gradient $\nabla f(x^\star) = 0$ but there is no reason for $\nabla f_i(x^\star)$ to vanish. {\color{blue} Consider Algorithm \ref{algo:sgd} with fixed stepsize $\gamma^k = \gamma$. Since, for any point $x^k$ the noise \eqref{eq:sg_variance} is separated from $0$ the point $x^\star$ is not fixed point of the algoithm.} This again (like for subgradient descent) leads to the slower rate than for the gradient descent, see the usual behaviour of convergence in Figure \ref{fig:sgd_vs_gd}.

\begin{figure}[H]
    \centering
    \input{basics_2/stoch_vs_grad}
    \caption{Evolution of iterates for GD with fixed stepsize $\gamma = \frac{2}{\mu + L}$ and SGD with decreasing stepsize $\gamma^k = \frac{1}{k}$ for $L$-smooth and $\mu$-strongly convex objective $f(x) = \|Ax-b\|_2^2$, with random generated $A\in\RR^{100\times100},b\in\RR^{100}$. As we could see from this plot, in the beginning, when the stepsizes in these two algorithm are approximately the same SGD performs better, however when $\gamma^k\ll \frac{1}{L}$ it slows down fast.}
    \label{fig:sgd_vs_gd}
\end{figure}

In \cite{solodov1998incremental, nedic2001convergence} authors propose SGD with a constant stepsize. In Proposition $2.4$ \cite{nedic2001convergence} it was proven that such algorithm has a convergence rate that could be split into two parts: first is dependent on the number of iteration and converges linearly (for the strongly convex objective) to $0$, though, the second part is independent on $k$ and does not go to $0$ that implies a linear convergence but only up to a fixed tolerance. In \cite{solodov1998incremental} the linear convergence takes place only under some additional assumptions about the relationship between $f_i$. In \cite{shalev2013stochastic} Stochastic Dual Coordinate Ascent (SDCA) was proposed, this algorithm has a linear convergence rate; however, it is limited to the finite sum problems with strongly convex $f_i$. This work was extended to the case when only the strong convexity of the global objective $f$ is required \cite{shalev2016sdca}.

% The key bottleneck in stochastic methods is a variance of the gradient, that requires to use the decreasing sequence of stepsizes to guarantee the convergence to the minimizer. 
Another possible solution is to decrease the variance of the gradient to guarantee the linear convergence for strongly convex objectives. One of the examples of such methods is SAG \cite{schmidt2017minimizing}. In this paper, authors propose a randomized variant of incremental aggregated gradient (IAG) \cite{blatt2007convergent} with the following iterations
$$
x^{k+1} = x^k - \frac{\gamma^k}{m}\sum_{i=1}^m y_i^k,
$$
where on every iteration a random training example $i^k$ is selected and 
$$
y_i^k = \begin{cases} \nabla f_i(x^k)~~~\textnormal{if }i = i^k,\\y_i^{k-1}~~~~~~~\,\textnormal{otherwise.}\end{cases}
$$
This algorithm combines the low cost of each iteration with the linear rate in the strongly convex case.
%  that is achieved because of keeping the most recent gradients for every component $f_i$ of the sum.
Memory allows usage of better approximations of the full gradient on each iteration that allows using constant stepsize. This algorithm was further extended to the composite set up SAGA in \cite{defazio2014saga}. Another example of variance reduction in stochastic gradient descent methods is method SVRG \cite{johnson2013accelerating}. Every iteration of this algorithm could be written as
$$
x^{k+1} = x^k -\gamma^k\left(\nabla f_i(x^k) - y_i^{k-1} + \frac{1}{m}\sum_{i=1}^m y_i\right),
$$
where $y_i$ is a last stored gradient for $f_i$. The key difference of this algorithm is in the way of updating gradients in the memory. Unlikely to SAG/SAGA where on every iteration one gradient is updated in SVRG gradients are updated simultaneously every $p$ iterations
$$
y_i = \nabla f_i(x^d)~~~~\textnormal{for all }i,
$$
where $d$ is a moment of the last update of stored in memory gradients. It is now understood that these algorithms are conceptually almost the same, and even they could be proven using the same proof \cite{kovalev2019don, hofmann2015variance}. Another class of methods is majorization-minimization algorithms Finito \cite{defazio2014finito}, MISO \cite{mairal2015incremental}, DAve-RPG (asynchronous version of MISO) \cite{mishchenko2018}. The majorization-minimization approach goes beyond optimization that provides a good start for designing new incremental methods \cite{qian2019miso}.