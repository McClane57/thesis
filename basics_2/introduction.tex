\section*{Introduction}\label{sec:basics_intro}
\addcontentsline{toc}{section}{Introduction}
The goal of machine learning is to learn the model from the provided data using some predefined algorithms. In other words, the goal of machine learners is to propose an algorithm (or a set of algorithms) that will be able to make the learning process without further human interventions. For example, in the binary classification problems, we would like to learn the binary function that classified every object of the specific set into one of two possible groups based on the attributes of the object. However, the set of functions from the set of attributes to $\{-1,1\}$ is so rich that for any given training set of examples, we could generate the function that will ideally suit the classification on this set by merely memorizing the correct label for every example. But such functions, generally, are not so good to predict the group of any new object as far as their behavior on such examples is usually an arbitrary random. Thus it is necessary to add some requirements on the nature of such functions in order to add some learning ability that will allow to use them for new (unknown during the learning process) examples \cite{vapnik2013nature}. One of the first examples was the Rosenblatt Perceptron \cite{rosenblatt1960perceptron}, where the author proposed to learn the separated hyperplane {between classes} as we could see in this work, the classification function is assumed to be linear. This assumption is one of the common in machine learning and could be used, for example, in the support vector machines (SVM) problem \cite{suykens1999least}. After fixing of the learning problem, the search for predictors could be formalized as a mathematical optimization problem.


Let us consider $m$ observations $(a_i, b_i)\in\mathcal{A}\times\mathcal{B}$ as an input of our prediction algorithm. The goal of prediction is to find prediction function $h(a, x)$, that belongs to some specific class and is parametrized by $x\in\RR^n$. Let us provide some examples of such functions
\begin{itemize}
    \item[] {\textbf{linear model}} $h(a,x) = x^\top \Phi(a)$, where features $\Phi(a)\in\RR^n$;
    \item[] {\textbf{neural networks}} $h(a,x) = x_l^\top\sigma(x_{l-1}^\top\sigma(\cdots x_1^\top a))$.
\end{itemize}

Now, let us introduce the loss function that is used as a measure of the quality of the prediction. Loss function $\ell(\cdot,\cdot)$ is a function that represents a distance to between two arguments and as a result is closer to $0$ if these arguments are closer to each other. Most widely used loss functions are 
\begin{itemize}
    \item[] {\textbf{quadratic loss}} $\ell(b, h(a,x)) = \frac{1}{2}(b - h(a,x))^2$,
    \item[] {\textbf{logistic loss}} $\ell(b, h(a,x)) = \log(1 + \exp(-bh(a,x))),$
\end{itemize}
where the quadratic loss is used in regression problems with $b\in\RR$ \cite{bhattacharya1966estimating} and logistic is used in classification problems with $b\in\{-1,1\}$ \cite{shao2014nonparallel, tang2004granular}.

As we already mentioned, the goal is to learn the best possible prediction function from the specific class of predictors. In other words, we need to find the vector of parameters $x\in\RR^n$ that minimizes the loss. However, we have access only to the finite amount of examples so we should consider empirical loss minimization (ERM) problem
\begin{equation}\label{eq:erm}
    \min_{x\in\RR^n} \sum_{i=1}^m \ell(b_i, h(a_i,x)).
\end{equation}
    

By the law of big numbers we know, that if the number of examples $m\rightarrow \infty$, then the empirical loss will converge to the real loss. This motivates machine learners to learn on as big training sets as possible that makes the problem extremely hard to solve (since $m$ is big). 

Another problem that appears in machine learning is overfitting, when a function is too closely fit to a limited set of data points (training set). One of the common technique to avoid overfitting is regularization that force the model to be simpler.
\begin{equation}\label{eq:erm_reg}
\min_{x\in\RR^n} \sum_{i=1}^m \ell(b_i, h(a_i,x)) + r(x).
\end{equation}

For example, in the variable selection problems it is important to have only few entries of estimator be non-zero in this case $\ell_1$ regularization is used ($r = \|\cdot\|_1$) to force the final solution to have few non-zero entries \cite{bach2012optimization}.

Solving (regularized) ERM problem is a complicated mathematical procedure that calls for different optimization techniques. In general, to simplify the optimization problem, loss functions are considered to be convex and smooth. For convex functions, any local minimum is a global one, and smoothness allows us to operate with the gradient or even higher derivatives to analyze the first(higher)-order approximation of the function. However, regularizers that enforce the specific structure to the final solution are usually convex but non-smooth. It calls for the optimization methods that could handle a non-smooth objective. In general, such methods are slower than their convex analogs since subgradients do not allow to make an equally good approximation of function as gradients, but often, regularizers have a simple geometrical structure that allows using proximal methods which reach the same rate of convergence as smooth ones.

In modern machine learning applications, the dimension of the problem $n$ and the amount of training examples $m$ are usually big. This makes the computation of the full gradient of the loss function expensive and unreliable. It moves state-of-the-art algorithms from gradient methods \cite{nesterov-book} to the incremental methods \cite{bertsekas2011incremental, bottou2010large}, where instead of full gradient computation on every iteration some unbiased estimator (stochastic gradient) is computed. These methods appear to be slow out of the box as far as a non-zero variance of stochastic gradients force to use decreasing stepsize that leads to a slower rate than for the standard full gradient methods. To figure it out in $2013$ in \cite{johnson2013accelerating}, stochastic variance reduced gradient method that allows using constant stepsize but requires more gradient computations that give all in all better convergence rate. {There is a wide variety of incremental methods, and we will make a short review in this section.}

Another way to make the optimization process faster is to make computations in parallel. Thanks to the sum-structure ERM and its gradient could be computed in parallel on different machines: each machine computes a gradient that corresponds to some subset of examples and after the results are aggregated to the full gradient\footnote{we are not going to go into details of these algorithms now as far as it will be the specific section in this work}. Together with computational speedup, these algorithms also bring a bottleneck: communication. When the dimension of the problem is big, the process of sending/receiving of the full gradient could be extremely costly and takes even more time than the computation of the gradient. During the last years, a lot of different algorithms that do data compression before sending were proposed \cite{alistarh2017qsgd, wangni2018gradient, hubara2017quantized, wang2017efficient}. In such methods, the way of compression does not use the structure of the problem, more precisely, the structure of the final solution. Let us consider $\ell_1$ regularized problem, so we a-priori know that the optimal solution is coordinate sparse. If we could know the set of coordinates that are non-zero in this solution, we could send only these coordinates of gradient and lose nothing in terms of the rate as far as it would correspond to projected gradient descent. The problem is: we never know this set of coordinates, but we could try to guess. 

As we mentioned above, usually regularizers have some strong geometrical properties that enforce the optimal solution to have a specific structure. Unfortunately, the exact pattern is unknown, but proximal point methods allow us to identify it \cite{nutini2019active}. However, this result sheds light on the moment when the iterate becomes sparse; it does not greenlight to switch to a projected mode. In contrast, the result of \cite{fadili2018sensitivity} says that for some proximal algorithms after unknown time, this projection technique would be legal. {This motivates us to do this research.}

\mitya{
\paragraph{Outline} This chapter is organized as follows. In section XXX, we recall the 
}