\section*{Introduction}\label{sec:soda-intro}
\addcontentsline{toc}{section}{Introduction}

\mitya{In this chapter, we propose an asynchronous distributed algorithm \spy~featuring a sparsification of upward communications (slave-to-master) of \dave~(see Section \ref{sec:basics_dave}. The sparsification mechanism consists in randomly zeroing of the local update entries. This randomized technique maintains the linear convergence in the mean-squared error sense for strongly convex objectives {{when difference in probabilities of coordinates to be selected is small enough}} and is adjustable to various levels of communication costs, machines' computational powers, and data distribution evenness. An attractive and original property of this algorithm is the possibility to use a fixed \dg{stepsize} that depends neither on communication delays nor on the number of machines.}

\mitya{We propose several options for the zeroing of coordinates.
First, we analyze \spyU: the version of \spy, where all the coordinates are selected independently and uniformly. We show that such sparsification performs worse than \dave.}

\mitya{Considering the identification property of \dave~(see Subsection \ref{sec:dave_ident}), we furthermore leverage on it to improve our sparsification technique by preferably sampling the entries in the support of the master model in the case of $\ell_1$-regularized problems performing \spyI. The good point about \spyI~is that when it converges, this approach can be seen as an automatic dimension reduction procedure, resulting in better performance in terms of \emph{quantity of information exchanged} that is better than for the \dave. Nevertheless, the theoretical analysis prevents us from using different probabilities in the case when the problem is ill-conditioned, and we show that in practice such algorithm could diverge.}


\mitya{To handle the divergence, we propose \spyS~Algorithm that allows using different probabilities; however, it requires scaling. This algorithm is proven to have the same theoretical rate as \spyU, but with the identification based selection (\spyI~with scaling) it performs better in practice.}

% Nevertheless, the theoretical analysis prevents us from using different probabilities in the case when the problem is ill-conditioned. Getting around this, we, first, analyze the theoretical and practical interest of the algorithm with \emph{uniform sparsification but still worse than \dave}.

\mitya{Finally, we investigate the performance of \spyI~in terms of \emph{expected convergence time} and show empirically that it performs better than \dave~both in terms of the \emph{amount of communications between machines} and \emph{time}.}

\vspace*{6pt}
\resizebox{0.96\linewidth}{!}{
\hspace*{-20pt}\begin{tabular}{|c|c|c|}
\hline
    Name & Reference & Description\\
    \hline
   \dave  & \cite{ICML18} &  delay-tolerant asynchronous proximal gradient descent\\
   \spy & Algorithm \ref{alg:spy} & general sparsification framework for \dave\\
   \spyU & Algorithm \ref{alg:uspy} & \spy~with uniform selection \\
   \spyI & Algorithm \ref{alg:soda} & \spy~with identification-based selection \\
   \spyS & Algorithm \ref{alg:sspy} & \spy~with scaled updates \\
   \spyIS & Algorithm \ref{alg:spydr} & \spy~with identification-based selection and scaled updates \\
 \hline
\end{tabular}}

\paragraph{Outline.}
\mitya{This chapter is organized as follows. In Section \ref{sec:distributed-sparse}, we present the general sparsification framework \spy~and investigate the convergence. In Section \ref{sec:distributed-adaptive}, we present the \spyU~and \spyS, prove the identification result for these algorithms, and show numerically that the performance is weaker than for \dave. In Section \ref{sec:i-spy}, we present \spyI~and show numerically that the algorithm could both converge and diverge in practice. Furthermore, we prove the identification result and better rate than for \dave~under the additional assumption on convergence. Finally in Subsection \ref{sec:exps}, we present our empirical investigation on the performance of \spyI.}


% idea of sparsification, recall the idea of adaptive selection for $\ell_1$ regularized problems and prove the identification result. In Chapter XXX, we present the reconditioned modification of our algorithm and in Chapter YYY, we investigate the practical aspects of sparsified algorithm with adaptive selection that has no theoretical guarantees to converge. The rest of this chapter is organized as follows. In Section \ref{sec:distributed-sparse} we present the general idea of \salgo algorithm. First, we introduce useful notations for sparsified algorithms. Then in Section \ref{sec:distributed-sparse-impl} we present the general algorithm and in Section \ref{sec:distributed-sparse-analyze} we investigate its convergence. In Section \ref{sec:distributed-adaptive} we discuss different options for the sparsification choice. First, we show that uniform coordinate selection is inefficient. After, we prove that when the probability gap is relatively small the algorithm identifies the near optimal support for $\ell_1$ regularized problems that motivates the adaptive coordinate selection. Finally, we present the scaled version of our algorithm \maskalgo that combines both adaptive selection and theoretical convergence; however, this algorithm is proven to be less efficient than \dave.

% Second, considering the identification property of \dave~(see Subsection \ref{sec:dave_ident}), we furthermore leverage on it to improve our sparsification technique by preferably sampling the entries in the support of the master model in the case of $\ell_1$-regularized problems. 



% This approach can be seen as an automatic dimension reduction procedure, resulting in better performance in terms of quantity of information exchanged {{that is proven to be better than the non-sparsified algorithm in terms of communications with specific probability for coordinates outside the support.}} The proposed method is the first asynchronous method with identification-based sparsification technique for $\ell_1$-regularized problems. Thus, all communications are eventually sparse.
% On the one hand, this sparsification could reach to the divergence of the algorithm. On the other hand, we theoretically prove, that in case of convergence, the performance of sparsified method is better than without sparsification both in terms of the \emph{amount of communications between machines} and \emp{time}.
% To handle the divergence issue, we propose the scaled version of the sparsified algorithm with non-uniform sampling. However, this approach performs worse both in theory and in practice.
% Finally, we show empirically that algorithm converges often, and the ``expected'' amount of communications between machines is smaller than without sparsification.

% such algorithms  and show empirically that algorithm converges often, and the ``expected'' amount of communications between machines is smaller than without sparsification. The second approach is in iterative proximal reconditioning of our problem. The same idea is used in the Catalyst acceleration scheme \cite{lin2015universal}, where the inner problem is well conditioned with any possible condition number.

% Moreover, in the case of $\ell_1$-regularized problems, we show that the generated master iterates identify some sparsity pattern after the finite time with probability one, resulting in sparse downward communications (master-to-slave). Thus, all communications are eventually sparse. 

% Finally, we furthermore leverage on this identification to improve our sparsification technique by preferably sampling the entries in the support of the master model; 
% this approach can be seen as an automatic dimension reduction procedure, resulting in better performance in terms of quantity of information exchanged {{that is proven to be better than the non-sparsified algorithm in terms of communications with specific probability for coordinates outside the support.}}
% \paragraph{Outline.}
% We split our work in three parts. In this chapter we present the general idea of sparsification, recall the idea of adaptive selection for $\ell_1$ regularized problems and prove the identification result. In Chapter XXX, we present the reconditioned modification of our algorithm and in Chapter YYY, we investigate the practical aspects of sparsified algorithm with adaptive selection that has no theoretical guarantees to converge. The rest of this chapter is organized as follows. In Section \ref{sec:distributed-sparse} we present the general idea of \salgo algorithm. First, we introduce useful notations for sparsified algorithms. Then in Section \ref{sec:distributed-sparse-impl} we present the general algorithm and in Section \ref{sec:distributed-sparse-analyze} we investigate its convergence. In Section \ref{sec:distributed-adaptive} we discuss different options for the sparsification choice. First, we show that uniform coordinate selection is inefficient. After, we prove that when the probability gap is relatively small the algorithm identifies the near optimal support for $\ell_1$ regularized problems that motivates the adaptive coordinate selection. Finally, we present the scaled version of our algorithm \maskalgo that combines both adaptive selection and theoretical convergence; however, this algorithm is proven to be less efficient than \dave.


% In Chapter \ref{ch:distributed}, we present the general theoretical analysis of \salgo with arbitrary probability vector $\pvec$. According to it, the convergence of \salgo is guaranteed only in case when the probabilities are relatively close to each other and this ``distance'' is smaller if the problem is less conditioned. However, the presented analysis works for any arbitrary probability vector that require it to consider the worst case. In practice, the adaptive selection that we propose is not arbitrary and we expect it to be better; however, the identification property takes place only for the algorithms that converges it is impossible to provide some better analysis using ``adaptivity'' of our sparsification.

% In this chapter, we investigate the practical aspects of \salgo algorithm with adaptive coordinate selection that we call \SP. In Section XXX, we prove the identification result under the additional assumption on the algorithm convergence. Also, we present the extended result on the rate of convergence and show that it is the same as for the algorithm without sparsification in terms of iteration, but is better in terms of data exchanged. In Section ZZZ, we present the numerical illustration of this. First, we see in practice, that the algorithm could diverge. It motivates us to introduce the ``expected convergence time'' over the runs, that is the average time of convergence for the successful runs scaled by the percentage of fails. We show that for considerably good probability selection, the ``expected convergence time'' is better for sparsified version than for \dave. Finally, we further investigate how big could be profit of sparsification in terms of time, iterations and communications and show that for relatively big datasets the time is better approximated by communications rather than by iterations that proves the practical importance of sparsification.