\section{Introduction}\label{sec:soda-intro}
In Chapter \ref{ch:distributed}, we present the general theoretical analysis of \salgo with arbitrary probability vector $\pvec$. According to it, the convergence of \salgo is guaranteed only in case when the probabilities are relatively close to each other and this ``distance'' is smaller if the problem is less conditioned. However, the presented analysis works for any arbitrary probability vector that require it to consider the worst case. In practice, the adaptive selection that we propose is not arbitrary and we expect it to be better; however, the identification property takes place only for the algorithms that converges it is impossible to provide some better analysis using ``adaptivity'' of our sparsification.

In this chapter, we investigate the practical aspects of \salgo algorithm with adaptive coordinate selection that we call \SP. In Section XXX, we prove the identification result under the additional assumption on the algorithm convergence. Also, we present the extended result on the rate of convergence and show that it is the same as for the algorithm without sparsification in terms of iteration, but is better in terms of data exchanged. In Section ZZZ, we present the numerical illustration of this. First, we see in practice, that the algorithm could diverge. It motivates us to introduce the ``expected convergence time'' over the runs, that is the average time of convergence for the successful runs scaled by the percentage of fails. We show that for considerably good probability selection, the ``expected convergence time'' is better for sparsified version than for \dave. Finally, we further investigate how big could be profit of sparsification in terms of time, iterations and communications and show that for relatively big datasets the time is better approximated by communications rather than by iterations that proves the practical importance of sparsification.