\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{vapnik2013nature}
\citation{rosenblatt1960perceptron}
\citation{suykens1999least}
\citation{bhattacharya1966estimating}
\citation{shao2014nonparallel}
\citation{tang2004granular}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}From Gradient to Identification}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:basics}{{1}{1}{From Gradient to Identification}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{1}{section.1.1}\protected@file@percent }
\newlabel{sec:basics_intro}{{1.1}{1}{Introduction}{section.1.1}{}}
\citation{bach2012optimization}
\citation{nesterov-book}
\citation{bertsekas2011incremental}
\citation{bottou2010large}
\citation{johnson2013accelerating}
\citation{alistarh2017qsgd}
\citation{wangni2018gradient}
\citation{hubara2017quantized}
\citation{wang2017efficient}
\newlabel{eq:erm}{{1.1}{2}{Introduction}{equation.1.1.1}{}}
\newlabel{eq:erm_reg}{{1.2}{2}{Introduction}{equation.1.1.2}{}}
\citation{nutini2019active}
\citation{fadili2018sensitivity}
\citation{hiriart2012fundamentals}
\citation{nesterov-book}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Convexity and smoothness}{3}{section.1.2}\protected@file@percent }
\newlabel{sec:basics_conv_and_smoothness}{{1.2}{3}{Convexity and smoothness}{section.1.2}{}}
\newlabel{eq:conv_def}{{1.3}{3}{Convex function}{equation.1.2.3}{}}
\newlabel{lm:convex}{{1.2}{3}{}{theorem.1.2}{}}
\newlabel{eq:convex_1}{{1.4}{3}{}{equation.1.2.4}{}}
\newlabel{eq:convex_2}{{1.5}{3}{}{equation.1.2.5}{}}
\citation{nesterov-book}
\newlabel{lm:descent}{{1.4}{4}{}{theorem.1.4}{}}
\newlabel{eq:descent_lemma_1}{{1.7}{4}{}{equation.1.2.7}{}}
\newlabel{eq:descent_lemma_2}{{1.8}{4}{}{equation.1.2.8}{}}
\newlabel{eq:lipsh}{{1.10}{4}{Convexity and smoothness}{equation.1.2.10}{}}
\citation{bubeck2015convex}
\@writefile{lof}{\contentsline {figure}{\numberline {1-1}{\ignorespaces Graphical illustration of lower and upper bound for $L$-smooth and $\mu $-strongly convex function $f:\mathbb  {R}\rightarrow \mathbb  {R}$\relax }}{5}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:functional_approximations}{{1-1}{5}{Graphical illustration of lower and upper bound for $L$-smooth and $\mu $-strongly convex function $f:\RR \rightarrow \RR $\relax }{figure.caption.4}{}}
\newlabel{lm:bubeck}{{1.5}{5}{}{theorem.1.5}{}}
\newlabel{eq:bubeck}{{1.12}{5}{}{equation.1.2.12}{}}
\citation{cauchy1847methode}
\citation{polyak1963gradient}
\citation{polyak1969minimization}
\citation{polyak1969conjugate}
\citation{nesterov2005smooth}
\citation{beck2009fast}
\citation{cauchy1847methode}
\citation{nesterov-book}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Gradient descent}{6}{section.1.3}\protected@file@percent }
\newlabel{sec:basics_gd}{{1.3}{6}{Gradient descent}{section.1.3}{}}
\newlabel{eq:pb_simple}{{1.17}{6}{Gradient descent}{equation.1.3.17}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Descent (GD)\relax }}{6}{algorithm.1}\protected@file@percent }
\newlabel{algo:gd}{{1}{6}{Gradient Descent (GD)\relax }{algorithm.1}{}}
\newlabel{th:gd}{{1.6}{6}{Convergence of Gradient Descent}{theorem.1.6}{}}
\newlabel{eq:gd_proof_1}{{1.20}{7}{Gradient descent}{equation.1.3.20}{}}
\newlabel{eq:gd_rate}{{1.22}{7}{Gradient descent}{equation.1.3.22}{}}
\citation{nesterov-book}
\newlabel{eq:gd_rate_strongly}{{1.23}{8}{Gradient descent}{equation.1.3.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Non-smooth optimization}{8}{section.1.4}\protected@file@percent }
\newlabel{sec:basics_nonsmooth}{{1.4}{8}{Non-smooth optimization}{section.1.4}{}}
\newlabel{eq:pb_r}{{1.24}{8}{Non-smooth optimization}{equation.1.4.24}{}}
\newlabel{eq:subgrad}{{1.25}{8}{Subgradient}{equation.1.4.25}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Subgradient Descent\relax }}{8}{algorithm.2}\protected@file@percent }
\newlabel{algo:sd}{{2}{8}{Subgradient Descent\relax }{algorithm.2}{}}
\citation{parikh2014proximal}
\@writefile{lof}{\contentsline {figure}{\numberline {1-2}{\ignorespaces Linear approximations of non-smooth function $r = \qopname  \relax m{max}\left \{-2x, (0.2x+1)^2 - 1\right \}$ from $\mathbb  {R}$ to $\mathbb  {R}$ at point $y = 0$ with different subgradients. As we could see, $y=0$ is a minimizer of $r$, however its subdifferential in this point is $\partial _0 r = [-2, 0.4]\ni 0$. Depending on the subgradient $g$ (we plot for $g=-1, -0.5, 0.4$) different linear approximations of function $r$ in $0$ appears and, as a result, the ``descent'' step is the descent step for these approximations but not for $r$.\relax }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:nonsmooth_approx}{{1-2}{9}{Linear approximations of non-smooth function $r = \max \left \{-2x, (0.2x+1)^2 - 1\right \}$ from $\RR $ to $\RR $ at point $y = 0$ with different subgradients. As we could see, $y=0$ is a minimizer of $r$, however its subdifferential in this point is $\partial _0 r = [-2, 0.4]\ni 0$. Depending on the subgradient $g$ (we plot for $g=-1, -0.5, 0.4$) different linear approximations of function $r$ in $0$ appears and, as a result, the ``descent'' step is the descent step for these approximations but not for $r$.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Proximal methods}{9}{section*.6}\protected@file@percent }
\newlabel{def:proximal_operator}{{1.8}{9}{Proximal operator}{theorem.1.8}{}}
\newlabel{lm:stationary_point}{{1.9}{9}{}{theorem.1.9}{}}
\citation{bauschke2011convex}
\citation{parikh2014proximal}
\newlabel{lm:firm}{{1.10}{10}{Firm nonexpansiveness of proximal operator}{theorem.1.10}{}}
\newlabel{eq:firm}{{1.27}{10}{Firm nonexpansiveness of proximal operator}{equation.1.4.27}{}}
\newlabel{lm:resolvent}{{1.11}{10}{Resolvent}{theorem.1.11}{}}
\newlabel{eq:resolvent}{{1.28}{10}{Resolvent}{equation.1.4.28}{}}
\citation{donoho1995noising}
\citation{rockafellar1976monotone}
\citation{bauschke2011convex}
\citation{moreau1962fonctions}
\citation{yosida2012functional}
\citation{yamada2011minimizing}
\@writefile{lof}{\contentsline {figure}{\numberline {1-3}{\ignorespaces Geometrical interpretation of soft-thresholding operator\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{fig:prox_l1}{{1-3}{11}{Geometrical interpretation of soft-thresholding operator\relax }{figure.caption.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Proximal Minimization\relax }}{11}{algorithm.3}\protected@file@percent }
\newlabel{algo:pm}{{3}{11}{Proximal Minimization\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Moreau-Yosida regularization}{11}{subsection.1.4.1}\protected@file@percent }
\newlabel{sec:basics_moreau-yosida}{{1.4.1}{11}{Moreau-Yosida regularization}{subsection.1.4.1}{}}
\newlabel{eq:M-Y}{{1.29}{11}{}{equation.1.4.29}{}}
\newlabel{eq:M-Y_grad}{{1.30}{11}{Moreau-Yosida regularization}{equation.1.4.30}{}}
\citation{rockafellar1976monotone}
\citation{lemarechal1997practical}
\@writefile{lof}{\contentsline {figure}{\numberline {1-4}{\ignorespaces Geometrical interpretation of Moreau envelope for $r = \qopname  \relax m{max}\left \{-x, 0.5x\right \}$ from $\mathbb  {R}$ to $\mathbb  {R}$.\relax }}{12}{figure.caption.8}\protected@file@percent }
\newlabel{fig:M-Y}{{1-4}{12}{Geometrical interpretation of Moreau envelope for $r = \max \left \{-x, 0.5x\right \}$ from $\RR $ to $\RR $.\relax }{figure.caption.8}{}}
\newlabel{eq:M-Y_smoothness}{{1.31}{12}{Moreau-Yosida regularization}{equation.1.4.31}{}}
\newlabel{eq:M-Y_strongly}{{1.32}{12}{Moreau-Yosida regularization}{equation.1.4.32}{}}
\newlabel{eq:M-Y_condition}{{1.33}{12}{Moreau-Yosida regularization}{equation.1.4.33}{}}
\citation{candes2008enhancing}
\citation{combettes2011proximal}
\citation{bach2012optimization}
\citation{daubechies2004iterative}
\citation{gabay1983chapter}
\citation{combettes2011proximal}
\citation{raguet2013generalized}
\citation{mairal2015incremental}
\citation{beck2009fast}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Composite optimization}{13}{subsection.1.4.2}\protected@file@percent }
\newlabel{sec:basics_composite}{{1.4.2}{13}{Composite optimization}{subsection.1.4.2}{}}
\newlabel{eq:pb_composite}{{1.34}{13}{Composite optimization}{equation.1.4.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{Proximal gradient descent}{13}{section*.9}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Proximal Gradient Descent (ISTA)\relax }}{13}{algorithm.4}\protected@file@percent }
\newlabel{algo:pgd}{{4}{13}{Proximal Gradient Descent (ISTA)\relax }{algorithm.4}{}}
\newlabel{eq:step_pgd}{{1.35}{13}{Proximal gradient descent}{equation.1.4.35}{}}
\citation{richtarik2012efficient}
\citation{nesterov2012efficiency}
\@writefile{toc}{\contentsline {subsubsection}{Coordinate descent methods}{14}{section*.10}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Coordinate Descent (CD) for \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:pb_simple}\unskip \@@italiccorr )}}\relax }}{14}{algorithm.5}\protected@file@percent }
\newlabel{algo:cd}{{5}{14}{Coordinate Descent (CD) for \eqref {eq:pb_simple}\relax }{algorithm.5}{}}
\citation{bach2012optimization}
\citation{richtarik2012efficient}
\citation{bach2012optimization}
\citation{hanzely2018sega}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Coordinate Descent (CD) for \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:pb_composite}\unskip \@@italiccorr )}}\relax }}{15}{algorithm.6}\protected@file@percent }
\newlabel{algo:cd_composite}{{6}{15}{Coordinate Descent (CD) for \eqref {eq:pb_composite}\relax }{algorithm.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Incremental methods}{15}{section*.11}\protected@file@percent }
\citation{kolmogorov1933grundbegriffe}
\citation{solodov1998incremental}
\citation{nedic2001convergence}
\citation{nedic2001convergence}
\citation{solodov1998incremental}
\citation{shalev2013stochastic}
\citation{shalev2016sdca}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Stochastic Gradient Descent (SGD)\relax }}{16}{algorithm.7}\protected@file@percent }
\newlabel{algo:sgd}{{7}{16}{Stochastic Gradient Descent (SGD)\relax }{algorithm.7}{}}
\newlabel{eq:sg_variance}{{1.36}{16}{Incremental methods}{equation.1.4.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1-5}{\ignorespaces Evolution of iterates for GD with fixed stepsize $\gamma = \frac  {2}{\mu + L}$ and SGD with decreasing stepsize $\gamma ^k = \frac  {1}{k}$ for $L$-smooth and $\mu $-strongly convex objective $f(x) = \delimiter "026B30D Ax-b\delimiter "026B30D _2^2$, with random generated $A\in \mathbb  {R}^{100\times 100},b\in \mathbb  {R}^{100}$. As we could see from this plot, in the beginning, when the stepsizes in these two algorithm are approximately the same SGD performs better, however when $\gamma ^k\ll \frac  {1}{L}$ it slows down fast.\relax }}{16}{figure.caption.12}\protected@file@percent }
\newlabel{fig:sgd_vs_gd}{{1-5}{16}{Evolution of iterates for GD with fixed stepsize $\gamma = \frac {2}{\mu + L}$ and SGD with decreasing stepsize $\gamma ^k = \frac {1}{k}$ for $L$-smooth and $\mu $-strongly convex objective $f(x) = \|Ax-b\|_2^2$, with random generated $A\in \RR ^{100\times 100},b\in \RR ^{100}$. As we could see from this plot, in the beginning, when the stepsizes in these two algorithm are approximately the same SGD performs better, however when $\gamma ^k\ll \frac {1}{L}$ it slows down fast.\relax }{figure.caption.12}{}}
\citation{schmidt2017minimizing}
\citation{blatt2007convergent}
\citation{defazio2014saga}
\citation{johnson2013accelerating}
\citation{kovalev2019don}
\citation{hofmann2015variance}
\citation{defazio2014finito}
\citation{mairal2015incremental}
\citation{mishchenko2018}
\citation{qian2019miso}
\citation{kolmogorov1933grundbegriffe}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Useful tools for stochastic processes analysis.}{17}{section.1.5}\protected@file@percent }
\newlabel{sec:basics-stoch-basics}{{1.5}{17}{Useful tools for stochastic processes analysis}{section.1.5}{}}
\newlabel{lm:tower_exp}{{1.13}{17}{Tower property}{theorem.1.13}{}}
\citation{stout1974almost}
\citation{robbins1971convergence}
\citation{vaiter2015model}
\citation{donoho1995noising}
\citation{wright1993identifiable}
\citation{hare2004identifying}
\citation{bertsekas1976goldstein}
\citation{burke1988identification}
\citation{lewis2002active}
\citation{drusvyatskiy2014optimality}
\citation{lewis2018partial}
\citation{fadili2018sensitivity}
\citation{wright2012accelerated}
\citation{poon2018local}
\citation{fadili2018model}
\citation{sun2019we}
\citation{ogawa2013safe}
\citation{fercoq2015mind}
\citation{liang2017activity}
\newlabel{th:r-s_theorem}{{1.15}{18}{}{theorem.1.15}{}}
\newlabel{eq:a-super-martingale}{{1.38}{18}{}{equation.1.5.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Identification}{18}{section.1.6}\protected@file@percent }
\newlabel{sec:basics_identificationn}{{1.6}{18}{Identification}{section.1.6}{}}
\citation{fadili2018sensitivity}
\citation{fadili2018sensitivity}
\citation{argyriou2013sparsity}
\citation{jenatton2011structured}
\citation{zhao2006grouped}
\newlabel{def:sparsity}{{1.16}{19}{Sparsity vector}{theorem.1.16}{}}
\newlabel{eq:strata}{{1.39}{19}{Sparsity vector}{equation.1.6.39}{}}
\newlabel{th:identification}{{1.17}{19}{Enlarged identification}{theorem.1.17}{}}
\newlabel{eq:identification_result}{{1.40}{19}{Enlarged identification}{equation.1.6.40}{}}
\newlabel{eq:l12}{{1.41}{19}{$\ell _{1,2}$ regularizer}{equation.1.6.41}{}}
\newlabel{eq:supp}{{1.42}{20}{$\ell _{1,2}$ regularizer}{equation.1.6.42}{}}
\newlabel{eq:ident_l1}{{1.43}{20}{$\ell _{1,2}$ regularizer}{equation.1.6.43}{}}
\newlabel{eq:s_to_supp}{{1.44}{20}{$\ell _{1,2}$ regularizer}{equation.1.6.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1-6}{\ignorespaces $\ell _1$ identification\relax }}{21}{figure.caption.13}\protected@file@percent }
\newlabel{fig:l1supp}{{1-6}{21}{$\ell _1$ identification\relax }{figure.caption.13}{}}
\newlabel{eq:tvreg}{{1.45}{21}{$\mathbf {TV}$ regularizer}{equation.1.6.45}{}}
\newlabel{eq:ident_tv}{{1.47}{21}{$\mathbf {TV}$ regularizer}{equation.1.6.47}{}}
\newlabel{eq:s_to_jumps}{{1.48}{22}{$\mathbf {TV}$ regularizer}{equation.1.6.48}{}}
\@setckpt{basics_2}{
\setcounter{page}{23}
\setcounter{equation}{48}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{0}
\setcounter{savepage}{0}
\setcounter{linenumber}{320}
\setcounter{LN@truepage}{25}
\setcounter{AM@survey}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{2}
\setcounter{bookmark@seq@number}{9}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{float@type}{16}
\setcounter{pp@next@reset}{0}
\setcounter{algorithm}{7}
\setcounter{ALC@unique}{34}
\setcounter{ALC@line}{5}
\setcounter{ALC@rem}{5}
\setcounter{ALC@depth}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{0}
\setcounter{algocfproc}{0}
\setcounter{algocf}{0}
\setcounter{tcbbreakpart}{0}
\setcounter{tcblayer}{0}
\setcounter{theorem}{19}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{section@level}{1}
}
