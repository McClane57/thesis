\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Background material}{4}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:basics}{{1}{4}{Background material}{chapter.1}{}}
\@writefile{toc}{\etoc@startlocaltoc{2}}
\citation{ICML18}
\@writefile{toc}{\contentsline {section}{Introduction}{5}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Outline.}{5}{section*.8}\protected@file@percent }
\citation{hiriart2012fundamentals}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Convex optimization}{6}{section.1.1}\protected@file@percent }
\newlabel{sec:basics_convex}{{1.1}{6}{Convex optimization}{section.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Convexity and smoothness}{6}{subsection.1.1.1}\protected@file@percent }
\newlabel{sec:basics_conv_and_smoothness}{{1.1.1}{6}{Convexity and smoothness}{subsection.1.1.1}{}}
\newlabel{eq:conv_def}{{1.1}{6}{Convex function}{equation.1.1.1}{}}
\citation{nesterov-book}
\newlabel{lm:convex}{{1.2}{7}{}{theorem.1.2}{}}
\newlabel{eq:convex_1}{{1.2}{7}{}{equation.1.1.2}{}}
\newlabel{eq:convex_2}{{1.3}{7}{}{equation.1.1.3}{}}
\citation{nesterov-book}
\newlabel{lm:descent}{{1.4}{8}{}{theorem.1.4}{}}
\newlabel{eq:descent_lemma_1}{{1.5}{8}{}{equation.1.1.5}{}}
\newlabel{eq:descent_lemma_2}{{1.6}{8}{}{equation.1.1.6}{}}
\newlabel{eq:lipsh}{{1.8}{8}{Convexity and smoothness}{equation.1.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1-1}{\ignorespaces Graphical illustration of lower and upper bound for $L$-smooth and $\mu $-strongly convex function $f:\mathbb  {R}\rightarrow \mathbb  {R}$\relax }}{9}{figure.caption.9}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:functional_approximations}{{1-1}{9}{Graphical illustration of lower and upper bound for $L$-smooth and $\mu $-strongly convex function $f:\RR \rightarrow \RR $\relax }{figure.caption.9}{}}
\citation{bubeck2015convex}
\newlabel{lm:bubeck}{{1.5}{10}{}{theorem.1.5}{}}
\newlabel{eq:bubeck}{{1.10}{10}{}{equation.1.1.10}{}}
\citation{cauchy1847methode}
\citation{polyak1963gradient}
\citation{polyak1969minimization}
\citation{polyak1969conjugate}
\citation{nesterov2005smooth}
\citation{beck2009fast}
\citation{cauchy1847methode}
\citation{nesterov-book}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Gradient descent}{11}{subsection.1.1.2}\protected@file@percent }
\newlabel{sec:basics_gd}{{1.1.2}{11}{Gradient descent}{subsection.1.1.2}{}}
\newlabel{eq:pb_simple}{{1.15}{11}{Gradient descent}{equation.1.1.15}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Descent (\texttt  {GD})\relax }}{12}{algorithm.1}\protected@file@percent }
\newlabel{algo:gd}{{1}{12}{Gradient Descent (\texttt {GD})\relax }{algorithm.1}{}}
\newlabel{th:gd}{{1.6}{12}{Convergence of Gradient Descent}{theorem.1.6}{}}
\newlabel{eq:gd_proof_1}{{1.18}{12}{Gradient descent}{equation.1.1.18}{}}
\newlabel{eq:gd_rate}{{1.20}{13}{Gradient descent}{equation.1.1.20}{}}
\newlabel{eq:gd_rate_strongly}{{1.21}{14}{Gradient descent}{equation.1.1.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Non-smooth optimization}{14}{subsection.1.1.3}\protected@file@percent }
\newlabel{sec:basics_nonsmooth}{{1.1.3}{14}{Non-smooth optimization}{subsection.1.1.3}{}}
\newlabel{eq:pb_r}{{1.22}{14}{Non-smooth optimization}{equation.1.1.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{Subgradient descent}{14}{section*.10}\protected@file@percent }
\newlabel{eq:subgrad}{{1.23}{14}{Subgradient}{equation.1.1.23}{}}
\citation{nesterov-book}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Subgradient Descent\relax }}{15}{algorithm.2}\protected@file@percent }
\newlabel{algo:sd}{{2}{15}{Subgradient Descent\relax }{algorithm.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1-2}{\ignorespaces Linear approximations of non-smooth function $r = \qopname  \relax m{max}\left \{-2x, (0.2x+1)^2 - 1\right \}$ from $\mathbb  {R}$ to $\mathbb  {R}$ at point $y = 0$ with different subgradients. As we could see, $y=0$ is a minimizer of $r$, however its subdifferential in this point is $\partial _0 r = [-2, 0.4]\ni 0$. Depending on the subgradient $g$ (we plot for $g=-1, -0.5, 0.4$) different linear approximations of function $r$ in $0$ appears and, as a result, the ``descent'' step is the descent step for these approximations but not for $r$.\relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{fig:nonsmooth_approx}{{1-2}{16}{Linear approximations of non-smooth function $r = \max \left \{-2x, (0.2x+1)^2 - 1\right \}$ from $\RR $ to $\RR $ at point $y = 0$ with different subgradients. As we could see, $y=0$ is a minimizer of $r$, however its subdifferential in this point is $\partial _0 r = [-2, 0.4]\ni 0$. Depending on the subgradient $g$ (we plot for $g=-1, -0.5, 0.4$) different linear approximations of function $r$ in $0$ appears and, as a result, the ``descent'' step is the descent step for these approximations but not for $r$.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{Proximal methods}{16}{section*.12}\protected@file@percent }
\newlabel{def:proximal_operator}{{1.8}{16}{Proximal operator}{theorem.1.8}{}}
\newlabel{eq:prox}{{1.24}{16}{Proximal operator}{equation.1.1.24}{}}
\citation{parikh2014proximal}
\citation{bauschke2011convex}
\newlabel{lm:stationary_point}{{1.9}{17}{}{theorem.1.9}{}}
\newlabel{lm:firm}{{1.10}{17}{Firm nonexpansiveness of proximal operator}{theorem.1.10}{}}
\newlabel{eq:firm}{{1.25}{17}{Firm nonexpansiveness of proximal operator}{equation.1.1.25}{}}
\citation{parikh2014proximal}
\citation{donoho1995noising}
\newlabel{lm:resolvent}{{1.11}{18}{Resolvent}{theorem.1.11}{}}
\newlabel{eq:resolvent}{{1.26}{18}{Resolvent}{equation.1.1.26}{}}
\citation{rockafellar1976monotone}
\citation{bauschke2011convex}
\citation{moreau1962fonctions}
\citation{yosida2012functional}
\@writefile{lof}{\contentsline {figure}{\numberline {1-3}{\ignorespaces Geometrical interpretation of soft-thresholding operator\relax }}{19}{figure.caption.13}\protected@file@percent }
\newlabel{fig:prox_l1}{{1-3}{19}{Geometrical interpretation of soft-thresholding operator\relax }{figure.caption.13}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Proximal Minimization\relax }}{19}{algorithm.3}\protected@file@percent }
\newlabel{algo:pm}{{3}{19}{Proximal Minimization\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Moreau-Yosida regularization}{19}{section*.14}\protected@file@percent }
\newlabel{sec:basics_moreau-yosida}{{1.1.3}{19}{Moreau-Yosida regularization}{section*.14}{}}
\citation{yamada2011minimizing}
\citation{rockafellar1976monotone}
\citation{lemarechal1997practical}
\newlabel{eq:M-Y}{{1.27}{20}{}{equation.1.1.27}{}}
\newlabel{eq:M-Y_grad}{{1.28}{20}{Moreau-Yosida regularization}{equation.1.1.28}{}}
\newlabel{eq:M-Y_smoothness}{{1.29}{20}{Moreau-Yosida regularization}{equation.1.1.29}{}}
\newlabel{eq:M-Y_strongly}{{1.30}{20}{Moreau-Yosida regularization}{equation.1.1.30}{}}
\citation{candes2008enhancing}
\citation{combettes2011proximal}
\citation{bach2012optimization}
\@writefile{lof}{\contentsline {figure}{\numberline {1-4}{\ignorespaces Geometrical interpretation of Moreau envelope for $r = \qopname  \relax m{max}\left \{-x, 0.5x\right \}$ from $\mathbb  {R}$ to $\mathbb  {R}$.\relax }}{21}{figure.caption.15}\protected@file@percent }
\newlabel{fig:M-Y}{{1-4}{21}{Geometrical interpretation of Moreau envelope for $r = \max \left \{-x, 0.5x\right \}$ from $\RR $ to $\RR $.\relax }{figure.caption.15}{}}
\newlabel{eq:M-Y_condition}{{1.31}{21}{Moreau-Yosida regularization}{equation.1.1.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Composite optimization}{21}{subsection.1.1.4}\protected@file@percent }
\newlabel{sec:basics_composite}{{1.1.4}{21}{Composite optimization}{subsection.1.1.4}{}}
\newlabel{eq:pb_composite}{{1.32}{21}{Composite optimization}{equation.1.1.32}{}}
\citation{daubechies2004iterative}
\citation{gabay1983chapter}
\citation{combettes2011proximal}
\citation{raguet2013generalized}
\citation{lange2000optimization}
\citation{cappe2009line}
\citation{neal1998view}
\citation{gasso2009recovering}
\citation{wright2009sparse}
\citation{mairal2015incremental}
\@writefile{toc}{\contentsline {subsubsection}{Proximal gradient descent}{22}{section*.16}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Proximal Gradient Descent (\texttt  {ISTA})\relax }}{22}{algorithm.4}\protected@file@percent }
\newlabel{algo:pgd}{{4}{22}{Proximal Gradient Descent (\texttt {ISTA})\relax }{algorithm.4}{}}
\newlabel{eq:step_pgd}{{1.33}{22}{Proximal gradient descent}{equation.1.1.33}{}}
\citation{beck2009fast}
\citation{richtarik2012efficient}
\citation{nesterov2012efficiency}
\@writefile{toc}{\contentsline {subsubsection}{Coordinate descent methods}{23}{section*.17}\protected@file@percent }
\newlabel{eq:lipsch_coord}{{1.34}{23}{Coordinate descent methods}{equation.1.1.34}{}}
\citation{wright2015coordinate}
\citation{bach2012optimization}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Coordinate Descent (\texttt  {CD}) for \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:pb_simple}\unskip \@@italiccorr )}}\relax }}{24}{algorithm.5}\protected@file@percent }
\newlabel{algo:cd}{{5}{24}{Coordinate Descent (\texttt {CD}) for \eqref {eq:pb_simple}\relax }{algorithm.5}{}}
\citation{richtarik2012efficient}
\citation{bach2012optimization}
\citation{hanzely2018sega}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Coordinate Descent (\texttt  {CD}) for \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:pb_composite}\unskip \@@italiccorr )}}\relax }}{25}{algorithm.6}\protected@file@percent }
\newlabel{algo:cd_composite}{{6}{25}{Coordinate Descent (\texttt {CD}) for \eqref {eq:pb_composite}\relax }{algorithm.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Incremental methods}{25}{section*.18}\protected@file@percent }
\citation{kolmogorov1933grundbegriffe}
\citation{shor1964structure}
\citation{nedic2001convergence}
\citation{solodov1998incremental}
\citation{shalev2013stochastic}
\citation{shalev2016sdca}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Stochastic Gradient Descent (\texttt  {SGD})\relax }}{26}{algorithm.7}\protected@file@percent }
\newlabel{algo:sgd}{{7}{26}{Stochastic Gradient Descent (\texttt {SGD})\relax }{algorithm.7}{}}
\newlabel{eq:sg_variance}{{1.35}{26}{Incremental methods}{equation.1.1.35}{}}
\citation{schmidt2017minimizing}
\citation{blatt2007convergent}
\@writefile{lof}{\contentsline {figure}{\numberline {1-5}{\ignorespaces Evolution of iterates for \texttt  {GD} with fixed stepsize $\gamma = \frac  {2}{\mu + L}$ and \texttt  {SGD} with decreasing stepsize $\gamma ^k = \frac  {1}{k}$ for $L$-smooth and $\mu $-strongly convex objective $f(x) = \|Ax-b\|_2^2$, with random generated $A\in \mathbb  {R}^{100\times 100},b\in \mathbb  {R}^{100}$. As we could see from this plot, in the beginning, when the stepsizes in these two algorithm are approximately the same \texttt  {SGD} performs better, however when $\gamma ^k\ll \frac  {1}{L}$ it slows down fast.\relax }}{27}{figure.caption.19}\protected@file@percent }
\newlabel{fig:sgd_vs_gd}{{1-5}{27}{Evolution of iterates for \texttt {GD} with fixed stepsize $\gamma = \frac {2}{\mu + L}$ and \texttt {SGD} with decreasing stepsize $\gamma ^k = \frac {1}{k}$ for $L$-smooth and $\mu $-strongly convex objective $f(x) = \|Ax-b\|_2^2$, with random generated $A\in \RR ^{100\times 100},b\in \RR ^{100}$. As we could see from this plot, in the beginning, when the stepsizes in these two algorithm are approximately the same \texttt {SGD} performs better, however when $\gamma ^k\ll \frac {1}{L}$ it slows down fast.\relax }{figure.caption.19}{}}
\citation{defazio2014saga}
\citation{johnson2013accelerating}
\citation{kovalev2019don}
\citation{hofmann2015variance}
\citation{defazio2014finito}
\citation{mairal2015incremental}
\citation{mishchenko2018}
\citation{qian2019miso}
\citation{vapnik2013nature}
\citation{rosenblatt1960perceptron}
\citation{suykens1999least}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Introduction to machine learning}{29}{section.1.2}\protected@file@percent }
\newlabel{sec:basics_intro}{{1.2}{29}{Introduction to machine learning}{section.1.2}{}}
\citation{bhattacharya1966estimating}
\citation{shao2014nonparallel}
\citation{tang2004granular}
\citation{vapnik2013nature}
\citation{bach2012optimization}
\newlabel{eq:loss}{{1.36}{30}{Introduction to machine learning}{equation.1.2.36}{}}
\newlabel{eq:erm}{{1.37}{30}{Introduction to machine learning}{equation.1.2.37}{}}
\newlabel{eq:erm_reg}{{1.38}{30}{Introduction to machine learning}{equation.1.2.38}{}}
\citation{nesterov-book}
\citation{bertsekas2011incremental}
\citation{bottou2010large}
\citation{johnson2013accelerating}
\citation{alistarh2017qsgd}
\citation{wangni2018gradient}
\citation{hubara2017quantized}
\citation{wang2017efficient}
\citation{nutini2019active}
\citation{fadili2018sensitivity}
\citation{Leslie91}
\citation{Kumar02}
\citation{konevcny2016federated}
\citation{konevcny2016federated2}
\citation{mishchenko2018}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Distributed learning}{32}{section.1.3}\protected@file@percent }
\newlabel{sec:distributed-intro}{{1.3}{32}{Distributed learning}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Computing setups}{32}{subsection.1.3.1}\protected@file@percent }
\newlabel{sec:basics_dist-set}{{1.3.1}{32}{Computing setups}{subsection.1.3.1}{}}
\citation{nedic2009distributed}
\citation{boyd2011distributed}
\citation{duchi2011dual}
\citation{shi2015extra}
\citation{tsitsiklis1984problems}
\citation{bertsekas1997parallel}
\citation{tsitsiklis1986distributed}
\citation{mishchenko2018}
\citation{BoydPCPE11}
\citation{Chen2016}
\citation{Tsianos12}
\citation{mishchenko2018}
\citation{yang2013trading}
\citation{dekel2012optimal}
\citation{shalev2013accelerated}
\citation{shamir2014distributed}
\citation{qu2016coordinate}
\citation{takavc2015distributed}
\citation{stich2018local}
\citation{khaled2019first}
\citation{khaled2020tighter}
\citation{li2019federated}
\citation{ma2017distributed}
\@writefile{toc}{\contentsline {subsubsection}{Synchronous algorithms}{33}{section*.20}\protected@file@percent }
\citation{wangni2018gradient}
\citation{leblond2016asaga}
\citation{pedregosa2017breaking}
\citation{peng2016arock}
\citation{alistarh2017qsgd}
\citation{horvath2019stochastic}
\citation{koloskova2019decentralized}
\citation{wen2017terngrad}
\citation{bernstein2018signsgd}
\citation{ben2019demystifying}
\citation{lin2017deep}
\citation{zhang2014asynchronous}
\citation{ma2015adding}
\citation{aytekin2016analysis}
\citation{peng2016arock}
\citation{calauzenes2017distributed}
\citation{li2013distributed}
\citation{konevcny2016federated}
\citation{ICML18}
\citation{hannah2016unbounded}
\citation{sun2017asynchronous}
\citation{ICML18}
\citation{mishchenko2018}
\citation{bertsekas1989parallel}
\@writefile{toc}{\contentsline {subsubsection}{Asynchronous algorithms}{34}{section*.21}\protected@file@percent }
\citation{mishchenko2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Notations and preliminaries}{35}{subsection.1.3.2}\protected@file@percent }
\newlabel{sec:distributed-prelim}{{1.3.2}{35}{Notations and preliminaries}{subsection.1.3.2}{}}
\newlabel{eq:pb}{{{$\mathsf  {P}$}}{35}{Notations and preliminaries}{subsection.1.3.2}{}}
\newlabel{eq:barx}{{1.39}{35}{Notations and preliminaries}{equation.1.3.39}{}}
\citation{vanli2016stronger}
\citation{mishchenko2018}
\citation{mishchenko2018}
\citation{ICML18}
\@writefile{lof}{\contentsline {figure}{\numberline {1-6}{\ignorespaces Notations of delays at iteration $k$.\relax }}{36}{figure.caption.22}\protected@file@percent }
\newlabel{fig:delays}{{1-6}{36}{Notations of delays at iteration $k$.\relax }{figure.caption.22}{}}
\newlabel{eq:km}{{1.40}{36}{Notations and preliminaries}{equation.1.3.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}\texttt  {DAve-PG}}{36}{subsection.1.3.3}\protected@file@percent }
\newlabel{sec:basics_dave}{{1.3.3}{36}{\dave }{subsection.1.3.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces \textsc  {\texttt  {DAve-PG}} on $((\alpha _i),(f_i),r)$ with stopping criterion $\mathsf  {C}$\relax }}{37}{algorithm.8}\protected@file@percent }
\newlabel{alg:dave}{{8}{37}{\textsc {\dave } on $((\alpha _i),(f_i),r)$ with stopping criterion $\mathsf {C}$\relax }{algorithm.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Convergence and rate for strongly convex objectives}{37}{section*.23}\protected@file@percent }
\citation{mishchenko2018}
\citation{wang2017efficient}
\citation{smith2015l1}
\citation{horvath2019stochastic}
\citation{yang2013trading}
\citation{khaled2019first}
\citation{mishchenko2018}
\citation{ICML18}
\newlabel{th:davepg}{{1.13}{38}{Th.~3.2~\cite {mishchenko2018}}{theorem.1.13}{}}
\newlabel{cor:bounded}{{1.14}{38}{Special case of bounded delays}{theorem.1.14}{}}
\citation{bach2012optimization}
\citation{vaiter2015model}
\citation{donoho1995noising}
\citation{wright1993identifiable}
\citation{hare2004identifying}
\@writefile{toc}{\contentsline {paragraph}{Discussion on communication.}{39}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Identification}{39}{section.1.4}\protected@file@percent }
\newlabel{sec:basics_identificationn}{{1.4}{39}{Identification}{section.1.4}{}}
\citation{bertsekas1976goldstein}
\citation{burke1988identification}
\citation{lewis2002active}
\citation{drusvyatskiy2014optimality}
\citation{lewis2018partial}
\citation{fadili2018sensitivity}
\citation{wright2012accelerated}
\citation{poon2018local}
\citation{fadili2018model}
\citation{sun2019we}
\citation{ogawa2013safe}
\citation{fercoq2015mind}
\citation{liang2017activity}
\citation{fadili2018sensitivity}
\citation{fadili2018sensitivity}
\@writefile{toc}{\contentsline {subsubsection}{Related literature}{40}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Active-set identification}{40}{subsection.1.4.1}\protected@file@percent }
\newlabel{def:sparsity}{{1.15}{40}{Sparsity vector}{theorem.1.15}{}}
\newlabel{eq:strata}{{1.41}{40}{Sparsity vector}{equation.1.4.41}{}}
\citation{argyriou2013sparsity}
\citation{jenatton2011structured}
\citation{zhao2006grouped}
\newlabel{th:identification}{{1.16}{41}{Enlarged identification}{theorem.1.16}{}}
\newlabel{eq:identification_result}{{1.42}{41}{Enlarged identification}{equation.1.4.42}{}}
\newlabel{eq:ident_union}{{1.43}{41}{Active-set identification}{equation.1.4.43}{}}
\@writefile{toc}{\contentsline {subsubsection}{Two examples}{41}{section*.26}\protected@file@percent }
\newlabel{ex:intro_coordinate_strata}{{1.17}{41}{$\ell _{1,2}$ regularizer}{theorem.1.17}{}}
\newlabel{eq:l12}{{1.44}{42}{$\ell _{1,2}$ regularizer}{equation.1.4.44}{}}
\newlabel{eq:supp}{{1.45}{42}{$\ell _{1,2}$ regularizer}{equation.1.4.45}{}}
\newlabel{eq:ident_l1}{{1.46}{42}{$\ell _{1,2}$ regularizer}{equation.1.4.46}{}}
\newlabel{eq:s_to_supp}{{1.47}{42}{$\ell _{1,2}$ regularizer}{equation.1.4.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1-7}{\ignorespaces $\ell _1$ identification\relax }}{43}{figure.caption.27}\protected@file@percent }
\newlabel{fig:l1supp}{{1-7}{43}{$\ell _1$ identification\relax }{figure.caption.27}{}}
\newlabel{ex:intro_jump_strata}{{1.18}{43}{$\mathbf {TV}$ regularizer}{theorem.1.18}{}}
\newlabel{eq:tvreg}{{1.48}{44}{$\mathbf {TV}$ regularizer}{equation.1.4.48}{}}
\newlabel{eq:ident_tv}{{1.50}{44}{$\mathbf {TV}$ regularizer}{equation.1.4.50}{}}
\newlabel{eq:s_to_jumps}{{1.51}{44}{$\mathbf {TV}$ regularizer}{equation.1.4.51}{}}
\newlabel{lm:exact_identification}{{1.19}{44}{Exact identification}{theorem.1.19}{}}
\newlabel{eq:qc}{{{QC}}{44}{Exact identification}{equation.1.4.52}{}}
\citation{lewis2002active}
\citation{vaiter2015model}
\newlabel{ex:exact_ident}{{1.21}{45}{Exact identification for $\ell _1$ regularizer}{theorem.1.21}{}}
\newlabel{eq:non-deg}{{{ND}}{45}{Exact identification for $\ell _1$ regularizer}{equation.1.4.52}{}}
\citation{ICML18}
\newlabel{eq:nondeg}{{1.52}{46}{Exact identification for $\ell _1$ regularizer}{equation.1.4.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Identification of \texttt  {DAve-PG}}{46}{subsection.1.4.2}\protected@file@percent }
\newlabel{sec:dave_ident}{{1.4.2}{46}{Identification of \dave }{subsection.1.4.2}{}}
\newlabel{cor:dave_ident}{{1.22}{46}{Identification of \dave }{theorem.1.22}{}}
\citation{bauschke2011convex}
\@setckpt{basics_2}{
\setcounter{page}{48}
\setcounter{equation}{52}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{3}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{0}
\setcounter{savepage}{0}
\setcounter{AM@survey}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{4}
\setcounter{bookmark@seq@number}{16}
\setcounter{parentequation}{0}
\setcounter{etoc@tocid}{2}
\setcounter{etoc@tocdepth}{2}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{float@type}{16}
\setcounter{pp@next@reset}{0}
\setcounter{algorithm}{8}
\setcounter{ALC@unique}{34}
\setcounter{ALC@line}{5}
\setcounter{ALC@rem}{5}
\setcounter{ALC@depth}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{0}
\setcounter{algocfproc}{0}
\setcounter{algocf}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{theorem}{22}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{section@level}{2}
}
