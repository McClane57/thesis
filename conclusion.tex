\chapter*{Conclusion and perspectives}
\addcontentsline{toc}{chapter}{Conclusion and perspectives}
\label{ch:conclusion}
\chaptermark{Conclusion and perspectives}

%\dg{In large-scale machine learning applications the dimension of the problem is sufficiently big.}
%In this thesis, we studied the problem that is common for all large-scale machine learning applications - huge dimensionality of the problem.
We propose a couple of techniques to reduce the dimensionality of the problem to reach a better convergence result both for distributed and non-distributed setups. In both cases, we focused on composite optimization problems with sparsity inducing regularizers.%; however this approach is common and widely used.

First, we consider the non-distributed setting where on every iteration of algorithm, we have an access to the whole dataset. It allows us to use vanilla proximal algorithm as a basic method for our a\dg{lg}orithm. Using the identification property of proximal methods for a wide class of regularizers, we propose \dg{a} ``sketch-and-project'' technique with specific identification-based selections of projections. This allows the acceleration of proximal gradient descent in terms of the amount of dimensions explored that we show both in theory and practice. Moreover, our technique allows using non-separable regularizers.
%that makes it field of usage bigger that for wide class of sketch and project algorithms.

Second, we present our approach for an asynchronous distributed master-worker setup. We consider \dg{the} problem of sparsification of distributed proximal algorithm by using the same identification-based idea. However, asynchronicity brings additional restrictions, so we consider only $\ell_1$ regularized problems in our study. Starting from a random sparsification that performs worse both in theory and practice, we propose two different approaches. In the first one, we investigate the practical performance of the algorithm with identification-based sparsification that is proven to converge only for well-conditioned problems. We show both that this algorithm could diverge, but in the same time, if it converges, it brings a significant performance profit. In the second one, we propose a proximal reconditioning technique that allows minimization of ill-conditioned problem via iterative approximate minimization of well-conditioned ones. It allows using our algorithm with theoretical guarantees to converge and good performance in practice.

This work opens several perspectives for future development. As we could see from the Figures \ref{fig:rcv1_1epoch} and \ref{fig:communication}, \ref{fig:speed_conv} the correlation between runtime and the amount of exchanges is better for \spyI~and it corresponds to the logic of the algorithm. As we could see from the theoretical results, the amount of iterations to solve the problem is larger if the sparsification is stronger. However, thanks to the identification property, the speed becomes faster and performance becomes exactly the same as for \dave. The situation with \recoalgo is not the same. On the one hand, identification takes place for this algorithm that allows solving the inner problem as fast as \dave. On the other hand, the proximal parameter $\rho$ is \dg{large} for \dg{small} $c$, \dg{which} makes the profit in terms of exchanges less significant than in \spyI~case. \dg{We could see from the numerical result,} the amount of iterations grows faster than the amount of communications decreases. \dg{This property} brings us to the ``sparsification limit'' \dg{(the smallest possible $c$ that leads to the acceleration)} and it could be further discovered in future works. 

Furthermore, in Remark \ref{rem:cata}, we mention the acceleration of algorithms in Nesterov's sense and Catalyst algorithm as an example of the accelerated proximal reconditioning technique. Since the dependence of amount of coordinates $c$ (as well as of proximal parameter $\rho$) in case of acceleration would be as a square root it could make the amount of required iterations for \spyI~ to converge much smaller and closer to \dave, or even smaller that could lead to the better convergence both in theory and practice. In addition, it would be interesting to compare performance of such accelerated method with performance of \spyI~when it converges.

Finally, one of the important open question is to design an efficient distributed version of Adaptive Randomized Proximal Subspace Descent, when each machine selects the subspace to project independently like in \cite{mishchenko201999}. Or even to investigate if the asynchronous option is possible.

\dg{In recent \cite{bareilles2019interplay} authors propose a method that is much more stable in terms of identification than accelerated proximal gradient descent and it is interesting to combine this method with our sparsification technique.}
% Another good question is different acceleration techniques that allow accelerating identification to make the algorithm faster due to the faster stabilization (we refer to \cite{bareilles2019interplay}).