\chapter{Introduction}
\label{ch:intro}

Mathematical optimization is one of the important tools in the Machine Learning. In supervised machine learning, the empirical risk minimization (ERM) is used to find the best approximator for the risk. This context is widely used in practice: in advertising, in marketing (for personal recommendations), in bioinformatics, in visual object recognition, and even photo gallery of your smartphone to recognize people, places, goods. In order to make better prediction large-scale data is used: the number of observations $m$ is large and the dimension of each observation (or the number of features) $n$ is big. It raises a host of questions, including, how to make the existing algorithms computationally more efficient? In this context, classical optimization method like gradient descent and its variations computationally expensive, because every step of these algorithm requires the pass through the full dataset. In contrast, incremental methods, that use a single data point or minibatch to compute an estimator for the gradient reduce the computational cost of iteration. However,  state-of-the-art algorithms are distributed to accept bigger datasets. In such algorithms, the communication process between machines is also expensive process and methods that could reduce the amount of communications or the size of every single update are reliable.

Another problem that appears in machine learning is overfitting. In other words, the best approximator (that minimizes empirical loss) is not the preferable one. For example, in the variable selection problems it is important to have only few entries of estimator be non-zero. Considering, that the solution of ERM problem has the arbitrary structure, this require for some changes in optimization problem to solve. On of the popular ways to enforce the specific structure to the final solution is to add a regularization term to the problem or to consider the constraint optimization problem, when the solution is forced to belong some specific set. Such regularizers are often non-smooth that calls for the optimization methods that could handle non-smooth objective. 


