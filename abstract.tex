% $Log: abstract.tex,v $
% Revision 1.1  93/05/14  14:56:25  starflt
% Initial revision
% 
% Revision 1.1  90/05/04  10:41:01  lwvanels
% Initial revision
% 
%
%% The text of your abstract and nothing else (other than comments) goes here.
%% It will be single-spaced and the rest of the text that is supposed to go on
%% the abstract page will be generated by the abstractpage environment.  This
%% file should be \input (not \include 'd) from cover.tex.
In this thesis, we develop \dg{a} framework to reduce the dimensionality of composite optimization problems with sparsity inducing regularizers.% We consider both distributed and non-distributed setups and propose an efficient framework that allows automatic-dimension reduction.
Based on the identification property of proximal methods, we first develop \dg{a} ``sketch-and-project'' method that uses projections based on the structure of the \dg{correct point}. \dg{This method} allows to work with random low-dimensional subspaces instead of considering the full space in the cases when the final solution is sparse. Second, \dg{we place ourselves in the context of} the delay-tolerant asynchronous proximal methods \dg{and} use our dimension reduction technique to decrease the total size of communications.
%by sparsifying the worker-to-master updates for $\ell_1$ regularized problems.
However, this technique is proven to converge only for well-conditioned problems both in theory in practice.
%but when it converges the performance is significantly better than without sparsification. 
\dg{Thus}, we investigate wrapping it up into \dg{a} proximal reconditioning framework. This leads to a theoretically \dg{backed} algorithm that is guaranteed to cost less in terms of communications compar\dg{ed} with \dg{a} non-sparsified version; \dg{we} show in practice that it implies faster runtime convergence when the \dg{sparsity} of the problem is sufficiently big.