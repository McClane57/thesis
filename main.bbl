\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{DGBSX12}

\bibitem[ABMP13]{argyriou2013sparsity}
Andreas Argyriou, Luca Baldassarre, Charles~A Micchelli, and Massimiliano
  Pontil.
\newblock On sparsity inducing regularization methods for machine learning.
\newblock In {\em Empirical inference}, pages 205--216. Springer, 2013.

\bibitem[AFJ16]{aytekin2016analysis}
Arda Aytekin, Hamid~Reza Feyzmahdavian, and Mikael Johansson.
\newblock Analysis and implementation of an asynchronous optimization algorithm
  for the parameter server.
\newblock {\em arXiv preprint arXiv:1610.05507}, 2016.

\bibitem[AGL{\etalchar{+}}17]{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem[B{\etalchar{+}}66]{bhattacharya1966estimating}
PK~Bhattacharya et~al.
\newblock Estimating the mean of a multivariate normal population with general
  quadratic loss function.
\newblock {\em The Annals of Mathematical Statistics}, 37(6):1819--1824, 1966.

\bibitem[B{\etalchar{+}}15]{bubeck2015convex}
S{\'e}bastien Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  8(3-4):231--357, 2015.

\bibitem[BC11]{bauschke2011convex}
Heinz~H Bauschke and Patrick~L Combettes.
\newblock {\em Convex analysis and monotone operator theory in Hilbert spaces}.
\newblock Springer Science \& Business Media, 2011.

\bibitem[Ber76]{bertsekas1976goldstein}
Dimitri~P Bertsekas.
\newblock On the goldstein-levitin-polyak gradient projection method.
\newblock {\em IEEE Transactions on automatic control}, 21(2):174--184, 1976.

\bibitem[Ber11]{bertsekas2011incremental}
Dimitri~P Bertsekas.
\newblock Incremental proximal methods for large scale convex optimization.
\newblock {\em Mathematical programming}, 129(2):163, 2011.

\bibitem[BHG07]{blatt2007convergent}
Doron Blatt, Alfred~O Hero, and Hillel Gauchman.
\newblock A convergent incremental gradient method with a constant step size.
\newblock {\em SIAM Journal on Optimization}, 18(1):29--51, 2007.

\bibitem[BJM{\etalchar{+}}12]{bach2012optimization}
Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, et~al.
\newblock Optimization with sparsity-inducing penalties.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  4(1):1--106, 2012.

\bibitem[BM88]{burke1988identification}
James~V Burke and Jorge~J Mor{\'e}.
\newblock On the identification of active constraints.
\newblock {\em SIAM Journal on Numerical Analysis}, 25(5):1197--1211, 1988.

\bibitem[BNH19]{ben2019demystifying}
Tal Ben-Nun and Torsten Hoefler.
\newblock Demystifying parallel and distributed deep learning: An in-depth
  concurrency analysis.
\newblock {\em ACM Computing Surveys (CSUR)}, 52(4):1--43, 2019.

\bibitem[Bot10]{bottou2010large}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In {\em Proceedings of COMPSTAT'2010}, pages 177--186. Springer,
  2010.

\bibitem[BPC{\etalchar{+}}11a]{boyd2011distributed}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et~al.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock {\em Foundations and Trends{\textregistered} in Machine learning},
  3(1):1--122, 2011.

\bibitem[BPC{\etalchar{+}}11b]{BoydPCPE11}
Stephen~P. Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock {\em Foundations and Trends in Machine Learning}, 3(1):1--122, 2011.

\bibitem[BT97]{bertsekas1997parallel}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock Parallel and distributed computation: numerical methods. 1989.
\newblock {\em Englewood Clifffs, New Jersey: Printice-Hall}, 1997.

\bibitem[BT09]{beck2009fast}
Amir Beck and Marc Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock {\em SIAM journal on imaging sciences}, 2(1):183--202, 2009.

\bibitem[BWAA18]{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock {\em arXiv preprint arXiv:1802.04434}, 2018.

\bibitem[Cau47]{cauchy1847methode}
Augustin Cauchy.
\newblock M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes
  d’{\'e}quations simultan{\'e}es.
\newblock {\em Comp. Rend. Sci. Paris}, 25(1847):536--538, 1847.

\bibitem[CMBJ16]{Chen2016}
Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal Jozefowicz.
\newblock Revisiting distributed synchronous sgd.
\newblock In {\em International Conference on Learning Representations Workshop
  Track}, 2016.

\bibitem[Con13]{condat2013direct}
Laurent Condat.
\newblock A direct algorithm for 1-d total variation denoising.
\newblock {\em IEEE Signal Processing Letters}, 20(11):1054--1057, 2013.

\bibitem[CP08]{combettes2008proximal}
Patrick~L Combettes and Jean-Christophe Pesquet.
\newblock Proximal thresholding algorithm for minimization over orthonormal
  bases.
\newblock {\em SIAM Journal on Optimization}, 18(4):1351--1376, 2008.

\bibitem[CP11]{combettes2011proximal}
Patrick~L Combettes and Jean-Christophe Pesquet.
\newblock Proximal splitting methods in signal processing.
\newblock In {\em Fixed-point algorithms for inverse problems in science and
  engineering}, pages 185--212. Springer, 2011.

\bibitem[CR17]{calauzenes2017distributed}
Cl{\'e}ment Calauz{\`e}nes and Nicolas~Le Roux.
\newblock Distributed saga: Maintaining linear convergence rate with limited
  communication.
\newblock {\em arXiv preprint arXiv:1705.10405}, 2017.

\bibitem[CWB08]{candes2008enhancing}
Emmanuel~J Candes, Michael~B Wakin, and Stephen~P Boyd.
\newblock Enhancing sparsity by reweighted l 1 minimization.
\newblock {\em Journal of Fourier analysis and applications}, 14(5-6):877--905,
  2008.

\bibitem[DAW11]{duchi2011dual}
John~C Duchi, Alekh Agarwal, and Martin~J Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock {\em IEEE Transactions on Automatic control}, 57(3):592--606, 2011.

\bibitem[DBLJ14]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem[DDC14]{defazio2014finito}
Aaron Defazio, Justin Domke, and Tiberio Caetano.
\newblock Finito: A faster, permutable incremental gradient method for big data
  problems.
\newblock In {\em Proceedings of the 31st international conference on machine
  learning (ICML-14)}, pages 1125--1133, 2014.

\bibitem[DDDM04]{daubechies2004iterative}
Ingrid Daubechies, Michel Defrise, and Christine De~Mol.
\newblock An iterative thresholding algorithm for linear inverse problems with
  a sparsity constraint.
\newblock {\em Communications on Pure and Applied Mathematics: A Journal Issued
  by the Courant Institute of Mathematical Sciences}, 57(11):1413--1457, 2004.

\bibitem[DGBSX12]{dekel2012optimal}
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.
\newblock Optimal distributed online prediction using mini-batches.
\newblock {\em Journal of Machine Learning Research}, 13(Jan):165--202, 2012.

\bibitem[DL14]{drusvyatskiy2014optimality}
Dmitriy Drusvyatskiy and Adrian~S Lewis.
\newblock Optimality, identifiability, and sensitivity.
\newblock {\em Mathematical Programming}, 147(1-2):467--498, 2014.

\bibitem[Don95]{donoho1995noising}
David~L Donoho.
\newblock De-noising by soft-thresholding.
\newblock {\em IEEE transactions on information theory}, 41(3):613--627, 1995.

\bibitem[DRT11]{dhillon2011nearest}
Inderjit~S Dhillon, Pradeep~K Ravikumar, and Ambuj Tewari.
\newblock Nearest neighbor based greedy coordinate descent.
\newblock In {\em Advances in Neural Information Processing Systems}, 2011.

\bibitem[FGMP18]{fadili2018model}
Jalal Fadili, Guillaume Garrigos, J{\'e}rome Malick, and Gabriel Peyr{\'e}.
\newblock Model consistency for learning with mirror-stratifiable regularizers.
\newblock {\em arXiv preprint arXiv:1803.08381}, 2018.

\bibitem[FGS15]{fercoq2015mind}
Olivier Fercoq, Alexandre Gramfort, and Joseph Salmon.
\newblock Mind the duality gap: safer rules for the lasso.
\newblock {\em arXiv preprint arXiv:1505.03410}, 2015.

\bibitem[FMP18]{fadili2018sensitivity}
Jalal Fadili, Jerome Malick, and Gabriel Peyr{\'e}.
\newblock Sensitivity analysis for mirror-stratifiable convex functions.
\newblock {\em SIAM Journal on Optimization}, 28(4):2975--3000, 2018.

\bibitem[FR15]{frongillo2015convergence}
Rafael Frongillo and Mark~D Reid.
\newblock Convergence analysis of prediction markets via randomized subspace
  descent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3034--3042, 2015.

\bibitem[Gab83]{gabay1983chapter}
Daniel Gabay.
\newblock Chapter ix applications of the method of multipliers to variational
  inequalities.
\newblock In {\em Studies in mathematics and its applications}, volume~15,
  pages 299--331. Elsevier, 1983.

\bibitem[GD13]{glasmachers2013accelerated}
Tobias Glasmachers and Urun Dogan.
\newblock Accelerated coordinate descent with adaptive coordinate frequencies.
\newblock In {\em Asian Conference on Machine Learning}, pages 72--86, 2013.

\bibitem[GIMA18]{grishchenko2018asynchronous}
Dmitry Grishchenko, Franck Iutzeler, J{\'e}r{\^o}me Malick, and Massih-Reza
  Amini.
\newblock Asynchronous distributed learning with sparse communications and
  identification.
\newblock {\em arXiv preprint arXiv:1812.03871}, 2018.

\bibitem[HCS{\etalchar{+}}17]{hubara2017quantized}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Quantized neural networks: Training neural networks with low
  precision weights and activations.
\newblock {\em The Journal of Machine Learning Research}, 18(1):6869--6898,
  2017.

\bibitem[HKM{\etalchar{+}}19]{horvath2019stochastic}
Samuel Horv{\'a}th, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and
  Peter Richt{\'a}rik.
\newblock Stochastic distributed learning with gradient quantization and
  variance reduction.
\newblock {\em arXiv preprint arXiv:1904.05115}, 2019.

\bibitem[HL04]{hare2004identifying}
Warren~L Hare and Adrian~S Lewis.
\newblock Identifying active constraints via partial smoothness and
  prox-regularity.
\newblock {\em Journal of Convex Analysis}, 11(2):251--266, 2004.

\bibitem[HLLJM15]{hofmann2015variance}
Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian McWilliams.
\newblock Variance reduced stochastic gradient descent with neighbors.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2305--2313, 2015.

\bibitem[HMR18]{hanzely2018sega}
Filip Hanzely, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock Sega: Variance reduction via gradient sketching.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2082--2093, 2018.

\bibitem[HUL12]{hiriart2012fundamentals}
Jean-Baptiste Hiriart-Urruty and Claude Lemar{\'e}chal.
\newblock {\em Fundamentals of convex analysis}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[HY16]{hannah2016unbounded}
Robert Hannah and Wotao Yin.
\newblock On unbounded delays in asynchronous parallel fixed-point algorithms.
\newblock {\em Journal of Scientific Computing}, pages 1--28, 2016.

\bibitem[JAB11]{jenatton2011structured}
Rodolphe Jenatton, Jean-Yves Audibert, and Francis Bach.
\newblock Structured variable selection with sparsity-inducing norms.
\newblock {\em Journal of Machine Learning Research}, 12(Oct):2777--2824, 2011.

\bibitem[JZ13]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in neural information processing systems}, pages
  315--323, 2013.

\bibitem[KHR19]{kovalev2019don}
Dmitry Kovalev, Samuel Horv{\'a}th, and Peter Richt{\'a}rik.
\newblock Don't jump through hoops and remove those loops: Svrg and katyusha
  are better without the outer loop.
\newblock {\em arXiv preprint arXiv:1901.08689}, 2019.

\bibitem[KMR19]{khaled2019first}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock First analysis of local gd on heterogeneous data.
\newblock {\em arXiv preprint arXiv:1909.04715}, 2019.

\bibitem[KMR20]{khaled2020tighter}
A~Khaled, K~Mishchenko, and P~Richt{\'a}rik.
\newblock Tighter theory for local sgd on identical and heterogeneous data.
\newblock In {\em The 23rd International Conference on Artificial Intelligence
  and Statistics (AISTATS 2020)}, 2020.

\bibitem[KMRR16]{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Daniel Ramage, and Peter
  Richt{\'a}rik.
\newblock Federated optimization: distributed machine learning for on-device
  intelligence.
\newblock {\em arXiv:1610.02527}, 2016.

\bibitem[KMY{\etalchar{+}}16]{konevcny2016federated2}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{\'a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock {\em arXiv:1610.05492}, 2016.

\bibitem[KOL33]{kolmogorov1933grundbegriffe}
AN~KOLMOGOROV.
\newblock Grundbegriffe der wahrscheinlichkeitreichnung.
\newblock {\em Ergebnisse der Mathematik}, 1933.

\bibitem[KSJ19]{koloskova2019decentralized}
Anastasia Koloskova, Sebastian~U Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock {\em arXiv preprint arXiv:1902.00340}, 2019.

\bibitem[Kum02]{Kumar02}
Vipin Kumar.
\newblock {\em Introduction to Parallel Computing}.
\newblock Addison-Wesley Longman, 2002.

\bibitem[LAS13]{li2013distributed}
Mu~Li, David~G Andersen, and Alexander Smola.
\newblock Distributed delayed proximal gradient methods.
\newblock In {\em NIPS Workshop on Optimization for Machine Learning}, 2013.

\bibitem[Lew02]{lewis2002active}
Adrian~S Lewis.
\newblock Active sets, nonsmoothness, and sensitivity.
\newblock {\em SIAM Journal on Optimization}, 13(3):702--725, 2002.

\bibitem[LFP17]{liang2017activity}
Jingwei Liang, Jalal Fadili, and Gabriel Peyr{\'e}.
\newblock Activity identification and local linear convergence of
  forward--backward-type methods.
\newblock {\em SIAM Journal on Optimization}, 27(1):408--437, 2017.

\bibitem[LHM{\etalchar{+}}17]{lin2017deep}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and William~J Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock {\em arXiv preprint arXiv:1712.01887}, 2017.

\bibitem[LL18]{lewis2018partial}
Adrian~S Lewis and Jingwei Liang.
\newblock Partial smoothness and constant rank.
\newblock {\em arXiv preprint arXiv:1807.03134}, 2018.

\bibitem[LMH15]{lin2015universal}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock In {\em Advances in neural information processing systems}, pages
  3384--3392, 2015.

\bibitem[LPLJ17]{leblond2016asaga}
R{\'e}mi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien.
\newblock Asaga: Asynchronous parallel saga.
\newblock In {\em Artificial Intelligence and Statistics}, pages 46--54, 2017.

\bibitem[LS97]{lemarechal1997practical}
Claude Lemar{\'e}chal and Claudia Sagastiz{\'a}bal.
\newblock Practical aspects of the moreau--yosida regularization: Theoretical
  preliminaries.
\newblock {\em SIAM Journal on Optimization}, 7(2):367--385, 1997.

\bibitem[LSS11]{loshchilov2011adaptive}
Ilya Loshchilov, Marc Schoenauer, and Mich{\`e}le Sebag.
\newblock Adaptive coordinate descent.
\newblock In {\em Proceedings of the 13th annual conference on Genetic and
  evolutionary computation}, pages 885--892. ACM, 2011.

\bibitem[LSTS19]{li2019federated}
Tian Li, Anit~Kumar Sahu, Ameet Talwalkar, and Virginia Smith.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock {\em arXiv preprint arXiv:1908.07873}, 2019.

\bibitem[Mai15]{mairal2015incremental}
Julien Mairal.
\newblock Incremental majorization-minimization optimization with application
  to large-scale machine learning.
\newblock {\em SIAM Journal on Optimization}, 25(2):829--855, 2015.

\bibitem[MGTR19]{mishchenko2019distributed}
Konstantin Mishchenko, Eduard Gorbunov, Martin Tak{\'a}{\v{c}}, and Peter
  Richt{\'a}rik.
\newblock Distributed learning with compressed gradient differences.
\newblock {\em arXiv preprint arXiv:1901.09269}, 2019.

\bibitem[MIM18]{mishchenko2018}
Konstantin Mishchenko, Franck Iutzeler, and J{\'e}r{\^o}me Malick.
\newblock A distributed flexible delay-tolerant proximal gradient algorithm.
\newblock {\em arXiv preprint arXiv:1806.09429}, 2018.

\bibitem[MIMA18]{ICML18}
Konstantin Mishchenko, Franck Iutzeler, J{\'e}r{\^o}me Malick, and Massih-Reza
  Amini.
\newblock A delay-tolerant proximal-gradient algorithm for distributed
  learning.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, volume~80, pages 3587--3595, 2018.

\bibitem[MKJ{\etalchar{+}}17]{ma2017distributed}
Chenxin Ma, Jakub Kone{\v{c}}n{\`y}, Martin Jaggi, Virginia Smith, Michael~I
  Jordan, Peter Richt{\'a}rik, and Martin Tak{\'a}{\v{c}}.
\newblock Distributed optimization with arbitrary local solvers.
\newblock {\em Optimization Methods and Software}, 32(4):813--848, 2017.

\bibitem[Mor62]{moreau1962fonctions}
Jean~Jacques Moreau.
\newblock Fonctions convexes duales et points proximaux dans un espace
  hilbertien.
\newblock 1962.

\bibitem[MSJ{\etalchar{+}}15]{ma2015adding}
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael Jordan, Peter Richtarik, and
  Martin Takac.
\newblock Adding vs. averaging in distributed primal-dual optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1973--1982, 2015.

\bibitem[NB01]{nedic2001convergence}
Angelia Nedi{\'c} and Dimitri Bertsekas.
\newblock Convergence rate of incremental subgradient algorithms.
\newblock In {\em Stochastic optimization: algorithms and applications}, pages
  223--264. Springer, 2001.

\bibitem[Nes05]{nesterov2005smooth}
Yu~Nesterov.
\newblock Smooth minimization of non-smooth functions.
\newblock {\em Mathematical programming}, 103(1):127--152, 2005.

\bibitem[Nes12]{nesterov2012efficiency}
Yu~Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock {\em SIAM Journal on Optimization}, 22(2):341--362, 2012.

\bibitem[Nes13]{nesterov-book}
Y.~Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[NLS17]{nutini2017let}
Julie Nutini, Issam Laradji, and Mark Schmidt.
\newblock Let's make block coordinate descent go fast: Faster greedy rules,
  message-passing, active-set complexity, and superlinear convergence.
\newblock {\em preprint arXiv:1712.08859}, 2017.

\bibitem[NO09]{nedic2009distributed}
Angelia Nedic and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock {\em IEEE Transactions on Automatic Control}, 54(1):48--61, 2009.

\bibitem[NP14]{necoara2014random}
Ion Necoara and Andrei Patrascu.
\newblock A random coordinate descent algorithm for optimization problems with
  composite objective function and linear coupled constraints.
\newblock {\em Computational Optimization and Applications}, 57(2):307--337,
  2014.

\bibitem[NSH19]{nutini2019active}
Julie Nutini, Mark Schmidt, and Warren Hare.
\newblock “active-set complexity” of proximal gradient: How long does it
  take to find the sparsity pattern?
\newblock {\em Optimization Letters}, 13(4):645--655, 2019.

\bibitem[NSL{\etalchar{+}}15]{nutini2015coordinate}
Julie Nutini, Mark Schmidt, Issam Laradji, Michael Friedlander, and Hoyt
  Koepke.
\newblock Coordinate descent converges faster with the gauss-southwell rule
  than random selection.
\newblock In {\em International Conference on Machine Learning}, 2015.

\bibitem[NSYD17]{namkoong2017adaptive}
Hongseok Namkoong, Aman Sinha, Steve Yadlowsky, and John~C Duchi.
\newblock Adaptive sampling probabilities for non-smooth optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  2574--2583, 2017.

\bibitem[OST13]{ogawa2013safe}
Kohei Ogawa, Yoshiki Suzuki, and Ichiro Takeuchi.
\newblock Safe screening of non-support vectors in pathwise svm computation.
\newblock In {\em International Conference on Machine Learning}, pages
  1382--1390, 2013.

\bibitem[OT06]{olshevsky2006convergence}
Alex Olshevsky and John~N Tsitsiklis.
\newblock Convergence rates in distributed consensus and averaging.
\newblock In {\em Proceedings of the 45th IEEE Conference on Decision and
  Control}, pages 3387--3392. IEEE, 2006.

\bibitem[OT09]{olshevsky2009convergence}
Alex Olshevsky and John~N Tsitsiklis.
\newblock Convergence speed in distributed consensus and averaging.
\newblock {\em SIAM Journal on Control and Optimization}, 48(1):33--55, 2009.

\bibitem[PB{\etalchar{+}}14]{parikh2014proximal}
Neal Parikh, Stephen Boyd, et~al.
\newblock Proximal algorithms.
\newblock {\em Foundations and Trends{\textregistered} in Optimization},
  1(3):127--239, 2014.

\bibitem[PCJ17]{perekrestenko2017faster}
Dmytro Perekrestenko, Volkan Cevher, and Martin Jaggi.
\newblock Faster coordinate descent via adaptive importance sampling.
\newblock {\em arXiv preprint arXiv:1703.02518}, 2017.

\bibitem[PLLJ17]{pedregosa2017breaking}
Fabian Pedregosa, R{\'e}mi Leblond, and Simon Lacoste-Julien.
\newblock Breaking the nonsmooth barrier: A scalable parallel method for
  composite optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  55--64, 2017.

\bibitem[PLS18]{poon2018local}
Clarice Poon, Jingwei Liang, and Carola-Bibiane Sch{\"o}nlieb.
\newblock Local convergence properties of saga/prox-svrg and acceleration.
\newblock {\em arXiv preprint arXiv:1802.02554}, 2018.

\bibitem[Pol63]{polyak1963gradient}
Boris~T Polyak.
\newblock Gradient methods for the minimisation of functionals.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  3(4):864--878, 1963.

\bibitem[Pol69a]{polyak1969conjugate}
Boris~T Polyak.
\newblock The conjugate gradient method in extremal problems.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  9(4):94--112, 1969.

\bibitem[Pol69b]{polyak1969minimization}
Boris~Teodorovich Polyak.
\newblock Minimization of unsmooth functionals.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  9(3):14--29, 1969.

\bibitem[PXYY16]{peng2016arock}
Zhimin Peng, Yangyang Xu, Ming Yan, and Wotao Yin.
\newblock Arock: an algorithmic framework for asynchronous parallel coordinate
  updates.
\newblock {\em SIAM Journal on Scientific Computing}, 38(5):A2851--A2879, 2016.

\bibitem[QR16]{qu2016coordinate}
Zheng Qu and Peter Richt{\'a}rik.
\newblock Coordinate descent with arbitrary sampling i: Algorithms and
  complexity.
\newblock {\em Optimization Methods and Software}, 31(5):829--857, 2016.

\bibitem[QSMR19]{qian2019miso}
Xun Qian, Alibek Sailanbayev, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock Miso is making a comeback with better proofs and rates.
\newblock {\em arXiv preprint arXiv:1906.01474}, 2019.

\bibitem[RFP13]{raguet2013generalized}
Hugo Raguet, Jalal Fadili, and Gabriel Peyr{\'e}.
\newblock A generalized forward-backward splitting.
\newblock {\em SIAM Journal on Imaging Sciences}, 6(3):1199--1226, 2013.

\bibitem[Roc76]{rockafellar1976monotone}
R~Tyrrell Rockafellar.
\newblock Monotone operators and the proximal point algorithm.
\newblock {\em SIAM journal on control and optimization}, 14(5):877--898, 1976.

\bibitem[Ros60]{rosenblatt1960perceptron}
Frank Rosenblatt.
\newblock Perceptron simulation experiments.
\newblock {\em Proceedings of the IRE}, 48(3):301--309, 1960.

\bibitem[RS71]{robbins1971convergence}
Herbert Robbins and David Siegmund.
\newblock A convergence theorem for non negative almost supermartingales and
  some applications.
\newblock In {\em Optimizing methods in statistics}, pages 233--257. Elsevier,
  1971.

\bibitem[RT12]{richtarik2012efficient}
Peter Richt{\'a}rik and Martin Tak{\'a}{\v{c}}.
\newblock Efficient serial and parallel coordinate descent methods for
  huge-scale truss topology design.
\newblock In {\em Operations Research Proceedings 2011}, pages 27--32.
  Springer, 2012.

\bibitem[RT14]{richtarik2014iteration}
Peter Richt{\'a}rik and Martin Tak{\'a}{\v{c}}.
\newblock Iteration complexity of randomized block-coordinate descent methods
  for minimizing a composite function.
\newblock {\em Mathematical Programming}, 144(1-2):1--38, 2014.

\bibitem[RT16]{richtarik2016optimal}
Peter Richt{\'a}rik and Martin Tak{\'a}{\v{c}}.
\newblock On optimal probabilities in stochastic coordinate descent methods.
\newblock {\em Optimization Letters}, 10(6):1233--1243, 2016.

\bibitem[SCD14]{shao2014nonparallel}
Yuan-Hai Shao, Wei-Jie Chen, and Nai-Yang Deng.
\newblock Nonparallel hyperplane support vector machine for binary
  classification problems.
\newblock {\em Information Sciences}, 263:22--35, 2014.

\bibitem[SFJJ16]{smith2015l1}
Virginia Smith, Simone Forte, Michael~I Jordan, and Martin Jaggi.
\newblock L1-regularized distributed optimization: A communication-efficient
  primal-dual framework.
\newblock {\em arXiv preprint arXiv:1512.04011, presented at the ML Systems
  Workshop of ICML}, 2016.

\bibitem[SHY17]{sun2017asynchronous}
Tao Sun, Robert Hannah, and Wotao Yin.
\newblock Asynchronous coordinate descent under more realistic assumptions.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem[SJNS19]{sun2019we}
Yifan Sun, Halyun Jeong, Julie Nutini, and Mark Schmidt.
\newblock Are we there yet? manifold identification of gradient-related
  proximal methods.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1110--1119, 2019.

\bibitem[SLRB17]{schmidt2017minimizing}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock {\em Mathematical Programming}, 162(1-2):83--112, 2017.

\bibitem[SLWY15]{shi2015extra}
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin.
\newblock Extra: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock {\em SIAM Journal on Optimization}, 25(2):944--966, 2015.

\bibitem[Sol98]{solodov1998incremental}
Mikhail~V Solodov.
\newblock Incremental gradient algorithms with stepsizes bounded away from
  zero.
\newblock {\em Computational Optimization and Applications}, 11(1):23--35,
  1998.

\bibitem[SRJ17]{stich2017safe}
Sebastian~U Stich, Anant Raj, and Martin Jaggi.
\newblock Safe adaptive importance sampling.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4381--4391, 2017.

\bibitem[SS14]{shamir2014distributed}
Ohad Shamir and Nathan Srebro.
\newblock Distributed stochastic optimization and learning.
\newblock In {\em 2014 52nd Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pages 850--857. IEEE, 2014.

\bibitem[SS16]{shalev2016sdca}
Shai Shalev-Shwartz.
\newblock Sdca without duality, regularization, and individual convexity.
\newblock In {\em International Conference on Machine Learning}, pages
  747--754, 2016.

\bibitem[SSZ13a]{shalev2013accelerated}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Accelerated mini-batch stochastic dual coordinate ascent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  378--385, 2013.

\bibitem[SSZ13b]{shalev2013stochastic}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock {\em Journal of Machine Learning Research}, 14(Feb):567--599, 2013.

\bibitem[Sti18]{stich2018local}
Sebastian~U Stich.
\newblock Local sgd converges fast and communicates little.
\newblock {\em arXiv preprint arXiv:1805.09767}, 2018.

\bibitem[Sto74]{stout1974almost}
William~F Stout.
\newblock Almost sure convergence.
\newblock 1974.

\bibitem[SV99]{suykens1999least}
Johan~AK Suykens and Joos Vandewalle.
\newblock Least squares support vector machine classifiers.
\newblock {\em Neural processing letters}, 9(3):293--300, 1999.

\bibitem[TBA86]{tsitsiklis1986distributed}
John Tsitsiklis, Dimitri Bertsekas, and Michael Athans.
\newblock Distributed asynchronous deterministic and stochastic gradient
  optimization algorithms.
\newblock {\em IEEE transactions on automatic control}, 31(9):803--812, 1986.

\bibitem[Teb18]{teboulle2018simplified}
Marc Teboulle.
\newblock A simplified view of first order methods for optimization.
\newblock {\em Mathematical Programming}, 170(1):67--96, 2018.

\bibitem[TJSZ04]{tang2004granular}
Yuchun Tang, Bo~Jin, Yi~Sun, and Yan-Qing Zhang.
\newblock Granular support vector machines for medical binary classification
  problems.
\newblock In {\em 2004 Symposium on Computational Intelligence in
  Bioinformatics and Computational Biology}, pages 73--78. IEEE, 2004.

\bibitem[TR12]{Tsianos12}
K.I. Tsianos and M.G. Rabbat.
\newblock Revisiting distributed synchronous sgd.
\newblock In {\em American Control Conference}, pages 1067--1072, 2012.

\bibitem[TRS15]{takavc2015distributed}
Martin Tak{\'a}{\v{c}}, Peter Richt{\'a}rik, and Nathan Srebro.
\newblock Distributed mini-batch sdca.
\newblock {\em arXiv preprint arXiv:1507.08322}, 2015.

\bibitem[Tse01]{tseng2001convergence}
Paul Tseng.
\newblock Convergence of a block coordinate descent method for
  nondifferentiable minimization.
\newblock {\em Journal of optimization theory and applications},
  109(3):475--494, 2001.

\bibitem[Tsi84]{tsitsiklis1984problems}
John~Nikolas Tsitsiklis.
\newblock Problems in decentralized decision making and computation.
\newblock Technical report, Massachusetts Inst of Tech Cambridge Lab for
  Information and Decision Systems, 1984.

\bibitem[TSR{\etalchar{+}}05]{tibshirani2005sparsity}
Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji~Zhu, and Keith Knight.
\newblock Sparsity and smoothness via the fused lasso.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 67(1):91--108, 2005.

\bibitem[Val90]{Leslie91}
Leslie~G. Valiant.
\newblock A bridging model for parallel computation.
\newblock {\em Communications of the ACM}, 33(8):103--111, 1990.

\bibitem[Vap13]{vapnik2013nature}
Vladimir Vapnik.
\newblock {\em The nature of statistical learning theory}.
\newblock Springer science \& business media, 2013.

\bibitem[VGFP15]{vaiter2015model}
Samuel Vaiter, Mohammad Golbabaee, Jalal Fadili, and Gabriel Peyr{\'e}.
\newblock Model selection with low complexity priors.
\newblock {\em Information and Inference: A Journal of the IMA}, 4(3):230--287,
  2015.

\bibitem[WKSZ17]{wang2017efficient}
Jialei Wang, Mladen Kolar, Nathan Srebro, and Tong Zhang.
\newblock Efficient distributed learning with sparsity.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3636--3645. JMLR. org, 2017.

\bibitem[Wri93]{wright1993identifiable}
Stephen~J Wright.
\newblock Identifiable surfaces in constrained optimization.
\newblock {\em SIAM Journal on Control and Optimization}, 31(4):1063--1079,
  1993.

\bibitem[Wri12]{wright2012accelerated}
Stephen~J Wright.
\newblock Accelerated block-coordinate relaxation for regularized optimization.
\newblock {\em SIAM Journal on Optimization}, 22(1):159--186, 2012.

\bibitem[Wri15]{wright2015coordinate}
Stephen~J Wright.
\newblock Coordinate descent algorithms.
\newblock {\em Mathematical Programming}, 151(1):3--34, 2015.

\bibitem[WWLZ18]{wangni2018gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1306--1316, 2018.

\bibitem[WXY{\etalchar{+}}17]{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In {\em Advances in neural information processing systems}, pages
  1509--1519, 2017.

\bibitem[Yan13]{yang2013trading}
Tianbao Yang.
\newblock Trading computation for communication: Distributed stochastic dual
  coordinate ascent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  629--637, 2013.

\bibitem[YLY11]{yuan2011efficient}
Lei Yuan, Jun Liu, and Jieping Ye.
\newblock Efficient methods for overlapping group lasso.
\newblock In {\em Advances in neural information processing systems}, pages
  352--360, 2011.

\bibitem[Yos12]{yosida2012functional}
K{\"o}saku Yosida.
\newblock {\em Functional analysis}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[YYY11]{yamada2011minimizing}
Isao Yamada, Masahiro Yukawa, and Masao Yamagishi.
\newblock Minimizing the moreau envelope of nonsmooth convex functions over the
  fixed point set of certain quasi-nonexpansive mappings.
\newblock In {\em Fixed-Point Algorithms for Inverse Problems in Science and
  Engineering}, pages 345--390. Springer, 2011.

\bibitem[ZK14]{zhang2014asynchronous}
Ruiliang Zhang and James Kwok.
\newblock Asynchronous distributed admm for consensus optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1701--1709, 2014.

\bibitem[ZRY06]{zhao2006grouped}
Peng Zhao, Guilherme Rocha, and Bin Yu.
\newblock Grouped and hierarchical model selection through composite absolute
  penalties.
\newblock {\em Department of Statistics, UC Berkeley, Tech. Rep}, 703, 2006.

\bibitem[ZZ15]{zhao2015stochastic}
Peilin Zhao and Tong Zhang.
\newblock Stochastic optimization with importance sampling for regularized loss
  minimization.
\newblock In {\em international conference on machine learning}, pages 1--9,
  2015.

\end{thebibliography}
